{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdiffeq import odeint\n",
    "from torchdiffeq import odeint_adjoint as odeint_adj\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from random import sample\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, twice_dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(twice_dim, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, twice_dim),\n",
    "            \n",
    "        )\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "                nn.init.constant_(m.bias,val=0)\n",
    "        \n",
    "    def forward(self, t, y):\n",
    "        \n",
    "#         print(y.shape)\n",
    "        out = self.net(y)\n",
    "        #out = torch.ones(y.shape)\n",
    "        #out[:,0] = y[:,2]\n",
    "        #out[:,1] = y[:,3]\n",
    "        #out[:,2] = y[:,0]/(0.6666**2)\n",
    "        #out[:,3] = y[:,1]/(0.3333**2)\n",
    "#         print(out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, momentum=0.99):\n",
    "        self.momentum = momentum\n",
    "        self.losses = []\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = None\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        if self.val is None:\n",
    "            self.avg = val\n",
    "        else:\n",
    "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
    "        self.val = val\n",
    "        self.log(val)\n",
    "    \n",
    "    def log(self, val):\n",
    "        self.losses.append(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianXD(q, p):\n",
    "    dim = q.shape[0]\n",
    "    sigma = np.ones(dim)\n",
    "    V = 0.5*np.sum(q**2 / (sigma**2))\n",
    "    grad = q/(sigma**2)\n",
    "    return V, grad\n",
    "\n",
    "def Shell2D(q, p):\n",
    "    r0 = np.sqrt(2)\n",
    "    sigma = 0.5\n",
    "    r = np.sqrt(np.dot(q, q))\n",
    "    V = abs(r-r0)/sigma\n",
    "    \n",
    "    if (r-r0) == 0 or r == 0:\n",
    "        grad = np.array([0, 0])\n",
    "    else:\n",
    "        grad = (q*(r-r0)/(sigma*r*abs(r-r0)))\n",
    "        \n",
    "    return V, grad\n",
    "\n",
    "def Wofe_Quapp(q, p):\n",
    "    x = q[0]\n",
    "    y = q[1]\n",
    "    V = x**4 + y**4 - 2*x**2 - 4*y**2 + x*y + 0.3*x + 0.1*y\n",
    "    grad = np.array([4*x**3 - 4*x + y + 0.3, 4*y**3 - 8*y + x + 0.1])\n",
    "    return V, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_mod(traj,batch_size,batch_length,dt):\n",
    "    \n",
    "    twice_dim = traj.size()[2]\n",
    "    sampled_is = torch.randint(traj.shape[0],size = (batch_size,))\n",
    "    sampled_js = torch.randint(traj.shape[1]-batch_length,size = (batch_size,))\n",
    "    initial_time = sampled_js*dt\n",
    "    \n",
    "    batch_t = torch.linspace(0.0,dt*(batch_length-1),batch_length)\n",
    "    pos_init = traj[sampled_is,sampled_js,:].view(-1, twice_dim)\n",
    "    \n",
    "    sampled_trajs = []\n",
    "    \n",
    "    for i in range(batch_size):          \n",
    "        sampled_trajs.append(traj[sampled_is[i],sampled_js[i]:sampled_js[i]+batch_length,:].view(-1,twice_dim))\n",
    "        \n",
    "    batch_trajs = torch.stack(sampled_trajs,dim=1)\n",
    "  \n",
    "    return batch_t,pos_init,batch_trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/state/partition1/slurm_tmp/12324331.0.0/ipykernel_2723/4101550548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mloss_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    197\u001b[0m                                \"of them.\")\n\u001b[1;32m    198\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torchdiffeq/_impl/adjoint.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *grad_y)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;31m# Run the augmented system backwards in time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 aug_state = odeint(\n\u001b[0m\u001b[1;32m    127\u001b[0m                     \u001b[0maugmented_dynamics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torchdiffeq/_impl/odeint.py\u001b[0m in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevent_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mevent_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate_until_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torchdiffeq/_impl/solvers.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_integrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0msolution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_advance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_advance\u001b[0;34m(self, next_t)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnext_t\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_num_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_num_steps exceeded ({}>={})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_num_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adaptive_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mn_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_interp_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp_coeff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_adaptive_step\u001b[0;34m(self, rk_state)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# trigger both. (i.e. interleaving them would be wrong.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_runge_kutta_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtableau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;31m# dtypes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# y1.dtype == self.y0.dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_runge_kutta_step\u001b[0;34m(func, y0, f0, t0, dt, t1, tableau)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mperturb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerturb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0myi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_UncheckedAssign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# Do nothing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_flat_to_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torchdiffeq/_impl/adjoint.py\u001b[0m in \u001b[0;36maugmented_dynamics\u001b[0;34m(t, y_aug)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0;31m# doesn't necessarily even exist if there is piecewise structure in time), so turning off gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0;31m# wrt t here means we won't compute that if we don't need it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0mfunc_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt_requires_grad\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0;31m# Workaround for PyTorch bug #39784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# Do nothing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/slurm_tmp/12324331.0.0/ipykernel_2723/3407618769.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         print(y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m#out = torch.ones(y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#out[:,0] = y[:,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3320/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Wofe Quapp\n",
    "training_trajs = torch.Tensor(np.load('wofe_quapp/trajs.npy'))\n",
    "twice_dim = training_trajs.size()[2]\n",
    "func = ODEFunc(twice_dim)\n",
    "\n",
    "niters = 1000\n",
    "optimizer = torch.optim.Adam(func.parameters(), lr=1e-2)\n",
    "loss_meter = RunningAverageMeter()\n",
    "\n",
    "nsample = 200\n",
    "sampleLength = 10\n",
    "dt = 0.1 \n",
    "\n",
    "for itr in range(1, niters + 1):\n",
    "    start = time.perf_counter()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch_t, batch_y0, batch_y = get_batch_mod(training_trajs, nsample, sampleLength,dt)\n",
    "\n",
    "    pred_y = odeint_adj(func, batch_y0, batch_t)\n",
    "\n",
    "    loss = torch.mean(torch.abs(pred_y - batch_y) )\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    loss_meter.update(loss.item())\n",
    "    \n",
    "    if itr % 10 == 0: # output log throughout\n",
    "        print('Iter: {}, running avg elbo: {:.4f}'.format(itr, loss_meter.avg))\n",
    "        print('current loss: {:.4f}'.format(loss_meter.val))\n",
    "        print('Last iteration took: ', time.perf_counter() - start)\n",
    "        \n",
    "    if itr % 10 == 0: # do a validation step across entire trajectory\n",
    "        for i in range(2):\n",
    "            plt.plot(pred_y[:,i,1].detach().numpy(),pred_y[:,i,0].detach().numpy(),color='blue',alpha=0.5)\n",
    "            plt.plot(batch_y[:,i,1].detach().numpy(),batch_y[:,i,0].detach().numpy(),color='red',alpha=0.5)\n",
    "        plt.savefig('temp/{0}.png'.format(itr))\n",
    "        plt.close()\n",
    "        \n",
    "# torch.save(func, 'wofe_quapp/model')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "func = torch.load('2d_shell/model')\n",
    "qs = [np.random.randn()*2 for i in range(twice_dim//2)]\n",
    "ps = [np.random.randn()*0.7 for i in range(twice_dim//2)]\n",
    "init = torch.tensor(qs+ps)\n",
    "dt = 0.1\n",
    "batch_length = 10\n",
    "batch_t = torch.linspace(0.0,dt*(batch_length-1),batch_length)\n",
    "pred_y = odeint_adj(func, init, batch_t)\n",
    "plt.plot(pred_y[:, 0].detach().numpy(),pred_y[:, 1].detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMC with Neural ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc_exact_den(nn.Module):\n",
    "    def __init__(self,ODE_model):\n",
    "        super(ODEFunc_exact_den, self).__init__()\n",
    "        self.net = ODE_model\n",
    "        \n",
    "    def forward(self, t, y):\n",
    "#         start = time.time()\n",
    "        #print(\"here\")\n",
    "        zero = torch.tensor([0.0]).view(-1,1)\n",
    "#         print('zero', zero.shape)\n",
    "#         print('y',y.shape)\n",
    "        inp = torch.cat((zero,y[:,:-1]),dim=1)\n",
    "#         print(inp)\n",
    "        f = self.net(t,y[:,:-1])\n",
    "        #print(time.time()-start)\n",
    "        Jacobian = torch.autograd.functional.jacobian(self.net, (t,y[:,:-1]))\n",
    "        #print(time.time()-start)\n",
    "        \n",
    "        Tr = torch.trace(Jacobian[1].view(twice_dim,twice_dim)).view(-1,1)\n",
    "        \n",
    "        out = torch.cat((f,-Tr),dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_H(f, q,p,M_inv):\n",
    "    V_o,grad = f(q,p)\n",
    "    H_o = V_o + 0.5*( np.matmul(p.reshape(1,-1),np.matmul(M_inv,p.reshape(-1,1)))   )\n",
    "    H_o = H_o.reshape(-1)\n",
    "    return H_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neural_dynamics(f,q_temp,p_temp,M,M_inv,steps,eps,store=True):\n",
    "    dim = q_temp.shape[0]\n",
    "    \n",
    "    batch_t = torch.linspace(0.0,eps*(steps-1),steps)\n",
    "    #print(batch_t.shape)\n",
    "    \n",
    "    q_temp = torch.tensor(q_temp)\n",
    "    p_temp = torch.tensor(p_temp)\n",
    "    Zero = torch.tensor([0.0])\n",
    "    init = torch.cat((q_temp,p_temp,Zero),dim=0).float()\n",
    "    init = init.view(1,-1)\n",
    "#     print(init.shape)\n",
    "    start = time.perf_counter()\n",
    "    pred_y = odeint(func_mod, init, batch_t,method = 'rk4',options = dict(step_size=eps))\n",
    "    print(time.perf_counter()-start)\n",
    "    #print(pred_y.shape)\n",
    "    \n",
    "    \n",
    "    pred_y = pred_y.detach().numpy()\n",
    "    p_y = pred_y[:,0,:-1]\n",
    "    J = pred_y[:,0,-1]\n",
    "    \n",
    "    energies = []\n",
    "    \n",
    "    for i in range(p_y.shape[0]):\n",
    "        q = p_y[i,:dim]\n",
    "        p = p_y[i,dim:]\n",
    "        curr_e = Evaluate_H(f,q,p,M_inv)\n",
    "    \n",
    "        energies.append(curr_e)\n",
    "    \n",
    "    energies = np.asarray(energies)\n",
    "    #print(energies.shape)\n",
    "    #print(p_y.shape)\n",
    "    return (p_y[-1,:dim],p_y[-1,dim:]),J[-1],p_y,energies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neural_HMC(f,q0,num_samples,eps,steps,store=False):\n",
    "    \n",
    "    samples = []\n",
    "    accept_rate = 0\n",
    "    q = q0\n",
    "    \n",
    "    if store == True:\n",
    "        stored_vals = np.zeros((num_samples,steps,twice_dim))\n",
    "        energies = np.zeros((num_samples,steps))\n",
    "        \n",
    "    ind = 0\n",
    "    M = np.identity(twice_dim//2)\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    while len(samples) < num_samples:\n",
    "        \n",
    "        ####### Need to fix\n",
    "        mean = np.zeros((twice_dim//2))\n",
    "        cov = M\n",
    "        p = np.random.multivariate_normal(mean, cov)\n",
    "        \n",
    "        q_temp = copy.deepcopy(q)\n",
    "        p_temp = copy.deepcopy(p)\n",
    "        \n",
    "        (q_f,p_f),J,path,energy = Neural_dynamics(f,q_temp,p_temp,M,M_inv,steps,eps,store=True)\n",
    "        stored_vals[ind,:,:] = np.asarray(path)\n",
    "        energies[ind,:] = np.asarray(energy).reshape(-1)\n",
    "        \n",
    "        p_f *= -1\n",
    "        \n",
    "        \n",
    "        V_o,grad = f(q,p)\n",
    "        V_f,grad = f(q_f,p_f)\n",
    "        H_o = V_o + 0.5*np.matmul(p.reshape(1,-1),np.matmul(M_inv,p.reshape(-1,1)))\n",
    "        H_o = H_o.reshape(-1)\n",
    "        H_f = V_f + 0.5*np.matmul(p_f.reshape(1,-1),np.matmul(M_inv,p_f.reshape(-1,1)))\n",
    "        H_f = H_f.reshape(-1)\n",
    "        \n",
    "        acceptance = (H_o - H_f) + J\n",
    "        \n",
    "        val = np.log(np.random.rand())\n",
    "        #print(J.shape)\n",
    "        #print(acceptance.shape)\n",
    "        \n",
    "        if val < acceptance:\n",
    "            q = q_f\n",
    "            accept_rate += 1\n",
    "        \n",
    "        samples.append(q)\n",
    "        \n",
    "        if len(samples)%500 == 0 and len(samples)< 2000:\n",
    "            if len(samples) == 500:\n",
    "                recent_samples = np.asarray(samples)\n",
    "            else:\n",
    "                recent_samples = np.asarray(samples[-500:])\n",
    "            if (np.abs(np.linalg.det(np.cov(recent_samples.T))) > 0.001):\n",
    "                M_inv = np.cov(recent_samples.T)\n",
    "                M = np.linalg.inv(M_inv)\n",
    "            else:\n",
    "                M = np.identity(twice_dim//2)\n",
    "                M_inv = np.linalg.inv(M)\n",
    "#             print(M_inv)\n",
    "        ind+=1\n",
    "             \n",
    "    acceptance = accept_rate/num_samples\n",
    "#     print(stored_vals.shape)\n",
    "#     print(energies.shape)\n",
    "    return samples,stored_vals,energies, acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12526560109108686\n",
      "0.12313965987414122\n",
      "0.12325534923002124\n",
      "0.12301078089512885\n",
      "0.123272375902161\n",
      "0.12268958007916808\n",
      "0.12585422699339688\n",
      "0.12579586915671825\n",
      "0.12463830993510783\n",
      "0.12592837819829583\n",
      "0.12540996610186994\n",
      "0.12236729194410145\n",
      "0.12290345318615437\n",
      "0.12273916299454868\n",
      "0.12300442601554096\n",
      "0.1232741370331496\n",
      "0.12334194383583963\n",
      "0.12302190694026649\n",
      "0.12315263785421848\n",
      "0.12284508091397583\n",
      "0.1232045260258019\n",
      "0.12270601885393262\n",
      "0.12320044497027993\n",
      "0.12304295995272696\n",
      "0.12316275108605623\n",
      "0.12296056607738137\n",
      "0.1231114468537271\n",
      "0.12294169492088258\n",
      "0.12317225011065602\n",
      "0.12287343898788095\n",
      "0.12306508398614824\n",
      "0.12273865099996328\n",
      "0.12300667096860707\n",
      "0.1226921018678695\n",
      "0.12506169988773763\n",
      "0.17239110404625535\n",
      "0.11836371198296547\n",
      "0.11711357487365603\n",
      "0.11747609893791378\n",
      "0.11726626683957875\n",
      "0.11734262481331825\n",
      "0.11679118918254972\n",
      "0.11730960407294333\n",
      "0.11681095603853464\n",
      "0.11705280491150916\n",
      "0.11684905318543315\n",
      "0.1171319920103997\n",
      "0.11708712088875473\n",
      "0.11727600498124957\n",
      "0.11690811906009912\n",
      "0.11795496311970055\n",
      "0.11716204416006804\n",
      "0.11782077699899673\n",
      "0.11851672106422484\n",
      "0.11775341304019094\n",
      "0.11696452787145972\n",
      "0.11757042305544019\n",
      "0.1171235260553658\n",
      "0.11731498991139233\n",
      "0.11714133084751666\n",
      "0.11763045797124505\n",
      "0.11701158690266311\n",
      "0.118028380908072\n",
      "0.11702160583809018\n",
      "0.11743326811119914\n",
      "0.11679790704511106\n",
      "0.11912107095122337\n",
      "0.12268722103908658\n",
      "0.12310201884247363\n",
      "0.12282604910433292\n",
      "0.11733687692321837\n",
      "0.12282705004326999\n",
      "0.12294662510976195\n",
      "0.12318354495801032\n",
      "0.12073243199847639\n",
      "0.12028499692678452\n",
      "0.1172675620764494\n",
      "0.1170455738902092\n",
      "0.11754978098906577\n",
      "0.11710503697395325\n",
      "0.11778313107788563\n",
      "0.1170385479927063\n",
      "0.11763836210593581\n",
      "0.11723229987546802\n",
      "0.1171874450519681\n",
      "0.11693686502985656\n",
      "0.11767547694034874\n",
      "0.11690024891868234\n",
      "0.11774998717010021\n",
      "0.11660066503100097\n",
      "0.11719846003688872\n",
      "0.11721012904308736\n",
      "0.11769005609676242\n",
      "0.11693685012869537\n",
      "0.11727834586054087\n",
      "0.11686468194238842\n",
      "0.11712310393340886\n",
      "0.11674649501219392\n",
      "0.11705618514679372\n",
      "0.11685917200520635\n",
      "100 1\n",
      "0.11701472289860249\n",
      "0.11674461397342384\n",
      "0.11725274287164211\n",
      "0.11676192609593272\n",
      "0.1174357000272721\n",
      "0.11670078593306243\n",
      "0.11762775713577867\n",
      "0.11678811814635992\n",
      "0.11765415593981743\n",
      "0.11700414307415485\n",
      "0.11750791408121586\n",
      "0.11715334584005177\n",
      "0.11723684589378536\n",
      "0.11718228599056602\n",
      "0.11716884421184659\n",
      "0.1167229029815644\n",
      "0.1194562769960612\n",
      "0.12127744406461716\n",
      "0.11738271987996995\n",
      "0.11699383100494742\n",
      "0.1173972690012306\n",
      "0.1168573647737503\n",
      "0.11763386987149715\n",
      "0.11696094903163612\n",
      "0.11733303987421095\n",
      "0.11703542992472649\n",
      "0.11758677009493113\n",
      "0.11677433084696531\n",
      "0.1175701420288533\n",
      "0.11728760390542448\n",
      "0.11766005191020668\n",
      "0.11703996197320521\n",
      "0.11758185108192265\n",
      "0.11673419689759612\n",
      "0.11756134196184576\n",
      "0.11710486211813986\n",
      "0.117601762060076\n",
      "0.11700795986689627\n",
      "0.11741200694814324\n",
      "0.1168899699114263\n",
      "0.11732482118532062\n",
      "0.11908617080189288\n",
      "0.1234841460827738\n",
      "0.12309444602578878\n",
      "0.12320047407411039\n",
      "0.12286127312108874\n",
      "0.12317535583861172\n",
      "0.12296706391498446\n",
      "0.12318326183594763\n",
      "0.12287363596260548\n",
      "0.12321882718242705\n",
      "0.12311418098397553\n",
      "0.12323718797415495\n",
      "0.12312283995561302\n",
      "0.12549861893057823\n",
      "0.12616421096026897\n",
      "0.12356545100919902\n",
      "0.12345382408238947\n",
      "0.1244496030267328\n",
      "0.1260540708899498\n",
      "0.12426921701990068\n",
      "0.12610099907033145\n",
      "0.22483577113598585\n",
      "0.11744096199981868\n",
      "0.1189741340931505\n",
      "0.12294396688230336\n",
      "0.12320183287374675\n",
      "0.12311028689146042\n",
      "0.1232359919231385\n",
      "0.12272619409486651\n",
      "0.12313837488181889\n",
      "0.12276888312771916\n",
      "0.12329066079109907\n",
      "0.1229037360753864\n",
      "0.1190756291616708\n",
      "0.12300210213288665\n",
      "0.12093698093667626\n",
      "0.12301261094398797\n",
      "0.12323083612136543\n",
      "0.12315605999901891\n",
      "0.12349051493220031\n",
      "0.12291112984530628\n",
      "0.11887968913652003\n",
      "0.12285290891304612\n",
      "0.12081783008761704\n",
      "0.12281783600337803\n",
      "0.1174753699451685\n",
      "0.12301201303489506\n",
      "0.11982363695278764\n",
      "0.1175953559577465\n",
      "0.12112200097180903\n",
      "0.12305491114966571\n",
      "0.12116867396980524\n",
      "0.12306498107500374\n",
      "0.11757473088800907\n",
      "0.12301252502948046\n",
      "0.11938473582267761\n",
      "0.12232663691975176\n",
      "0.11747602908872068\n",
      "0.1171086358372122\n",
      "100 2\n",
      "0.1170091899111867\n",
      "0.12244535516947508\n",
      "0.12338082795031369\n",
      "0.12293814099393785\n",
      "0.12331220088526607\n",
      "0.12303777993656695\n",
      "0.12303838389925659\n",
      "0.12300934083759785\n",
      "0.12318374612368643\n",
      "0.1229091938585043\n",
      "0.12165091396309435\n",
      "0.12290006806142628\n",
      "0.12270667497068644\n",
      "0.11693759192712605\n",
      "0.11725277197547257\n",
      "0.11667198711074889\n",
      "0.1174056869931519\n",
      "0.11688129603862762\n",
      "0.11729563516564667\n",
      "0.1167528738733381\n",
      "0.1174180640373379\n",
      "0.116845817072317\n",
      "0.11755634099245071\n",
      "0.11691741505637765\n",
      "0.11734569002874196\n",
      "0.11802244489081204\n",
      "0.12096733809448779\n",
      "0.12274266593158245\n",
      "0.11806415091268718\n",
      "0.12179374904371798\n",
      "0.12302549299784005\n",
      "0.11678594700060785\n",
      "0.12076878384687006\n",
      "0.11678443616256118\n",
      "0.11985316011123359\n",
      "0.1229172779712826\n",
      "0.12351768906228244\n",
      "0.12373028299771249\n",
      "0.12062464188784361\n",
      "0.11959424684755504\n",
      "0.11817687703296542\n",
      "0.11781691201031208\n",
      "0.11803630203939974\n",
      "0.11792593309655786\n",
      "0.11784460791386664\n",
      "0.11785688600502908\n",
      "0.11819743481464684\n",
      "0.11789952497929335\n",
      "0.1180248640011996\n",
      "0.11752266390249133\n",
      "0.11811240785755217\n",
      "0.11775664705783129\n",
      "0.11794065404683352\n",
      "0.11770375305786729\n",
      "0.11807140102609992\n",
      "0.11786425206810236\n",
      "0.11792081804014742\n",
      "0.11766041186638176\n",
      "0.11783538106828928\n",
      "0.11790774809196591\n",
      "0.1179552678950131\n",
      "0.11771355918608606\n",
      "0.11808896600268781\n",
      "0.11789073306135833\n",
      "0.11800599005073309\n",
      "0.1177965400274843\n",
      "0.11821739701554179\n",
      "0.11777923791669309\n",
      "0.11821189895272255\n",
      "0.11794100399129093\n",
      "0.11826032400131226\n",
      "0.11791167687624693\n",
      "0.11817599087953568\n",
      "0.12165641994215548\n",
      "0.1245375091675669\n",
      "0.11950173485092819\n",
      "0.11805239296518266\n",
      "0.11772470083087683\n",
      "0.11971367197111249\n",
      "0.12992640095762908\n",
      "0.12504547298885882\n",
      "0.12060748087242246\n",
      "0.11811082996428013\n",
      "0.11803321586921811\n",
      "0.11827510804869235\n",
      "0.1179692919831723\n",
      "0.11851439694873989\n",
      "0.11804293585009873\n",
      "0.11807784996926785\n",
      "0.11797428200952709\n",
      "0.11818463588133454\n",
      "0.11769658210687339\n",
      "0.11879877909086645\n",
      "0.11782269994728267\n",
      "0.11808920395560563\n",
      "0.11818863498046994\n",
      "0.11828143987804651\n",
      "0.11801314703188837\n",
      "0.11837660591118038\n",
      "0.11778440116904676\n",
      "100 3\n",
      "0.11762869800440967\n",
      "0.11779970000497997\n",
      "0.11813202709890902\n",
      "0.11782123916782439\n",
      "0.11791907995939255\n",
      "0.1176706850528717\n",
      "0.11790100811049342\n",
      "0.11766239488497376\n",
      "0.11791744595393538\n",
      "0.11762847495265305\n",
      "0.1178216990083456\n",
      "0.11768898810259998\n",
      "0.11801283690147102\n",
      "0.11771002993918955\n",
      "0.11787055502645671\n",
      "0.1176807819865644\n",
      "0.11785211693495512\n",
      "0.11776148504577577\n",
      "0.11806811112910509\n",
      "0.1177878889720887\n",
      "0.11853008694015443\n",
      "0.1178453341126442\n",
      "0.11804636707529426\n",
      "0.11814032611437142\n",
      "0.11825499893166125\n",
      "0.11815109103918076\n",
      "0.1182846708688885\n",
      "0.11788834095932543\n",
      "0.11832726304419339\n",
      "0.11785331601276994\n",
      "0.11797834211029112\n",
      "0.11767368693836033\n",
      "0.11845149379223585\n",
      "0.11804505391046405\n",
      "0.1185758940409869\n",
      "0.11791114998050034\n",
      "0.11806692997924984\n",
      "0.11773046688176692\n",
      "0.11774285486899316\n",
      "0.11785314488224685\n",
      "0.11799165094271302\n",
      "0.11790340300649405\n",
      "0.11807147203944623\n",
      "0.1177548780106008\n",
      "0.11812305403873324\n",
      "0.11787990597076714\n",
      "0.11800195486284792\n",
      "0.1176676859613508\n",
      "0.1179322109092027\n",
      "0.11786612402647734\n",
      "0.11819735891185701\n",
      "0.11786791682243347\n",
      "0.11797069595195353\n",
      "0.11790055618621409\n",
      "0.11790033406578004\n",
      "0.11785547505132854\n",
      "0.1180470040999353\n",
      "0.11776083894073963\n",
      "0.11806954187341034\n",
      "0.1178833618760109\n",
      "0.11799475806765258\n",
      "0.1261101341806352\n",
      "0.11929641012102365\n",
      "0.11818126402795315\n",
      "0.11801676801405847\n",
      "0.11779264407232404\n",
      "0.11789528606459498\n",
      "0.11797749390825629\n",
      "0.11796906986273825\n",
      "0.11811763606965542\n",
      "0.11811420693993568\n",
      "0.11842238111421466\n",
      "0.1183480469044298\n",
      "0.11786261410452425\n",
      "0.11817673686891794\n",
      "0.11791740101762116\n",
      "0.11811616900376976\n",
      "0.11785138095729053\n",
      "0.11812297813594341\n",
      "0.11772482702508569\n",
      "0.11810907791368663\n",
      "0.11782329785637558\n",
      "0.12139096204191446\n",
      "0.120737724006176\n",
      "0.11895118886604905\n",
      "0.11800252599641681\n",
      "0.118118389043957\n",
      "0.117856987984851\n",
      "0.11818609503097832\n",
      "0.11793859396129847\n",
      "0.11816618987359107\n",
      "0.11782049015164375\n",
      "0.11813797312788665\n",
      "0.11807780480012298\n",
      "0.11806516302749515\n",
      "0.11788510205224156\n",
      "0.11806929693557322\n",
      "0.11770855402573943\n",
      "0.1181462500244379\n",
      "0.11770515702664852\n",
      "100 4\n",
      "0.11777292983606458\n",
      "0.11773473490029573\n",
      "0.118337397929281\n",
      "0.11838493589311838\n",
      "0.11808779207058251\n",
      "0.11771155684255064\n",
      "0.11843071202747524\n",
      "0.11787532805465162\n",
      "0.11867550085298717\n",
      "0.11775214201770723\n",
      "0.11805204581469297\n",
      "0.11798636196181178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11792129604145885\n",
      "0.11819076398387551\n",
      "0.1179813239723444\n",
      "0.11783470399677753\n",
      "0.1185903549194336\n",
      "0.1178729438688606\n",
      "0.11805210285820067\n",
      "0.11778539814986289\n",
      "0.11784394807182252\n",
      "0.11770803993567824\n",
      "0.11794277606531978\n",
      "0.11755163897760212\n",
      "0.1180789889767766\n",
      "0.1177451650146395\n",
      "0.11784058483317494\n",
      "0.11787595110945404\n",
      "0.11799003300257027\n",
      "0.11777928797528148\n",
      "0.11794365383684635\n",
      "0.11776681593619287\n",
      "0.11793166003189981\n",
      "0.1177504351362586\n",
      "0.11785899614915252\n",
      "0.11772775906138122\n",
      "0.11803224380128086\n",
      "0.11861638003028929\n",
      "0.1338058360852301\n",
      "0.13359368895180523\n",
      "0.13387061189860106\n",
      "0.13336888002231717\n",
      "0.1337541388347745\n",
      "0.1476215859875083\n",
      "0.1393699620384723\n",
      "0.13341027707792819\n",
      "0.13388644205406308\n",
      "0.13356177089735866\n",
      "0.13373932498507202\n",
      "0.133484975900501\n",
      "0.1337915000040084\n",
      "0.12070732819847763\n",
      "0.11809192108921707\n",
      "0.11793583096005023\n",
      "0.1180905089713633\n",
      "0.1176407269667834\n",
      "0.11809810996055603\n",
      "0.11779496213421226\n",
      "0.11780621204525232\n",
      "0.11773173790425062\n",
      "0.1178085261490196\n",
      "0.11769202002324164\n",
      "0.11794325592927635\n",
      "0.11775325704365969\n",
      "0.11792262899689376\n",
      "0.11763018602505326\n",
      "0.11787454318255186\n",
      "0.11790569499135017\n",
      "0.11785454885102808\n",
      "0.11771793803200126\n",
      "0.11790541396476328\n",
      "0.11759082600474358\n",
      "0.1179830760229379\n",
      "0.11779370903968811\n",
      "0.11799520906060934\n",
      "0.11772229406051338\n",
      "0.11809605988673866\n",
      "0.11780819110572338\n",
      "0.1181863381061703\n",
      "0.11771265091374516\n",
      "0.11809006216935813\n",
      "0.11807459313422441\n",
      "0.11801812401972711\n",
      "0.1185635388828814\n",
      "0.11849491600878537\n",
      "0.11810887418687344\n",
      "0.11824459815397859\n",
      "0.11787831084802747\n",
      "0.11821690993383527\n",
      "0.11839833203703165\n",
      "0.1180430050007999\n",
      "0.1177306689787656\n",
      "0.11850561504252255\n",
      "0.11803171690553427\n",
      "0.11818879703059793\n",
      "0.1177178320940584\n",
      "0.11807510093785822\n",
      "0.11874638590961695\n",
      "0.11835975688882172\n",
      "0.11773279402405024\n",
      "100 5\n",
      "61.39212997816503\n"
     ]
    }
   ],
   "source": [
    "# Wofe-Quapp\n",
    "num_samples = [100]#[10000] #[100, 200, 500, 1000, 2000]\n",
    "twice_dim = 4\n",
    "init = np.random.randn(twice_dim//2)*2\n",
    "traj_length = 50\n",
    "traj_step_size = 0.1\n",
    "num_of_runs = 5\n",
    "potential = 'wofe_quapp'\n",
    "potential_function = Wofe_Quapp\n",
    "func = torch.load(f'{potential}/model')\n",
    "func_mod = ODEFunc_exact_den(func)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for q in num_samples:\n",
    "    for i in range(1, num_of_runs+1):\n",
    "        samps,trajs,energies,acceptance = Neural_HMC(potential_function, init, q, traj_step_size, traj_length, store=True)\n",
    "#         if not os.path.exists(f'{potential}/neural_{q}'):\n",
    "#             os.makedirs(f'{potential}/neural_{q}')\n",
    "\n",
    "#         with open(f'{potential}/neural_{q}/{i}_hmc_samps.npy', 'wb') as f:\n",
    "#             np.save(f, samps)\n",
    "\n",
    "#         with open(f'{potential}/neural_{q}/{i}_info.npy', 'wb') as f:\n",
    "#             np.save(f, np.array([time.perf_counter() - start, acceptance]))\n",
    "        print(q, i)\n",
    "print(time.perf_counter() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02663175412453711\n",
      "0.025933255907148123\n",
      "0.02590642007999122\n",
      "0.025735458126291633\n",
      "0.02570769004523754\n",
      "0.025539154885336757\n",
      "0.025528655154630542\n",
      "0.025447018910199404\n",
      "0.02525609196163714\n",
      "0.025536427041515708\n",
      "0.04388742009177804\n",
      "0.04853394185192883\n",
      "0.0487692030146718\n",
      "0.04875711095519364\n",
      "0.04871633090078831\n",
      "0.025638798950240016\n",
      "0.025569788878783584\n",
      "0.02552951592952013\n",
      "0.025578740052878857\n",
      "0.025522694922983646\n",
      "0.02586206584237516\n",
      "0.02556145517155528\n",
      "0.02552804001607001\n",
      "0.025540111120790243\n",
      "0.02552183298394084\n",
      "0.025450980057939887\n",
      "0.025523700984194875\n",
      "0.02551606297492981\n",
      "0.02579379896633327\n",
      "0.02547114691697061\n",
      "0.02552872896194458\n",
      "0.025502959033474326\n",
      "0.025570247089490294\n",
      "0.02561549306847155\n",
      "0.025544589152559638\n",
      "0.025559185072779655\n",
      "0.025827277917414904\n",
      "0.02554623782634735\n",
      "0.025537763023748994\n",
      "0.025506204925477505\n",
      "0.025560795096680522\n",
      "0.025539816124364734\n",
      "0.025506882928311825\n",
      "0.025533751817420125\n",
      "0.02586555597372353\n",
      "0.02549196081236005\n",
      "0.02554635307751596\n",
      "0.025513854110613465\n",
      "0.025579303968697786\n",
      "0.025518673937767744\n",
      "0.025528619065880775\n",
      "0.0255626467987895\n",
      "0.02578911604359746\n",
      "0.02554499381221831\n",
      "0.02616505208425224\n",
      "0.026226952904835343\n",
      "0.02622663090005517\n",
      "0.026178629137575626\n",
      "0.026262163883075118\n",
      "0.026254936819896102\n",
      "0.026443065144121647\n",
      "0.02624115697108209\n",
      "0.02621089993044734\n",
      "0.026215557008981705\n",
      "0.02620752900838852\n",
      "0.026227982016280293\n",
      "0.026239510159939528\n",
      "0.026255459990352392\n",
      "0.026418818859383464\n",
      "0.026205410016700625\n",
      "0.026276824064552784\n",
      "0.026192255085334182\n",
      "0.026192798046395183\n",
      "0.026189264142885804\n",
      "0.026131819933652878\n",
      "0.026198108913376927\n",
      "0.02644410403445363\n",
      "0.026248617097735405\n",
      "0.026227579917758703\n",
      "0.026173824910074472\n",
      "0.026225987123325467\n",
      "0.02620975999161601\n",
      "0.026203345973044634\n",
      "0.026240172097459435\n",
      "0.026079569943249226\n",
      "0.026237080805003643\n",
      "0.026193583151325583\n",
      "0.02620821911841631\n",
      "0.025574352825060487\n",
      "0.026221496053040028\n",
      "0.026234193006530404\n",
      "0.02617687894962728\n",
      "0.026298402110114694\n",
      "0.026232026983052492\n",
      "0.02570565603673458\n",
      "0.026267691049724817\n",
      "0.026216495083644986\n",
      "0.02623664797283709\n",
      "0.025589052122086287\n",
      "0.026178680825978518\n",
      "0.026517707854509354\n",
      "0.026223873952403665\n",
      "0.02592780697159469\n",
      "0.026275771902874112\n",
      "0.026231512892991304\n",
      "0.026236369973048568\n",
      "0.025598190957680345\n",
      "0.026201836997643113\n",
      "0.026483048917725682\n",
      "0.026168758049607277\n",
      "0.026213178876787424\n",
      "0.025773183908313513\n",
      "0.026214312063530087\n",
      "0.026200369000434875\n",
      "0.026223627850413322\n",
      "0.025540311122313142\n",
      "0.02658796194009483\n",
      "0.026216287864372134\n",
      "0.026090706000104547\n",
      "0.026245424058288336\n",
      "0.02624562685377896\n",
      "0.025545486016198993\n",
      "0.026204840978607535\n",
      "0.0261618010699749\n",
      "0.02613373612985015\n",
      "0.02623792109079659\n",
      "0.026223032968118787\n",
      "0.026178468950092793\n",
      "0.025560386944562197\n",
      "0.02623863611370325\n",
      "0.026261725928634405\n",
      "0.02555294195190072\n",
      "0.026474064914509654\n",
      "0.026173816062510014\n",
      "0.026162307942286134\n",
      "0.025571773992851377\n",
      "0.026174975791946054\n",
      "0.02625355403870344\n",
      "0.026247164933010936\n",
      "0.025681463070213795\n",
      "0.026452929014340043\n",
      "0.026155389845371246\n",
      "0.026165916118770838\n",
      "0.02558996295556426\n",
      "0.02618726110085845\n",
      "0.02621343289501965\n",
      "0.026199361076578498\n",
      "0.02551294700242579\n",
      "0.0264605600386858\n",
      "0.02618206897750497\n",
      "0.025642887922003865\n",
      "0.026090560015290976\n",
      "0.026318585965782404\n",
      "0.026225058129057288\n",
      "0.025553674902766943\n",
      "0.026191971031948924\n",
      "0.02622685208916664\n",
      "0.026193591067567468\n",
      "0.025624054949730635\n",
      "0.02620619209483266\n",
      "0.026135811116546392\n",
      "0.02615131693892181\n",
      "0.025579138891771436\n",
      "0.02618999290280044\n",
      "0.026418257039040327\n",
      "0.025904644979164004\n",
      "0.025665068067610264\n",
      "0.025488812010735273\n",
      "0.02555573987774551\n",
      "0.025522385025396943\n",
      "0.025493421824648976\n",
      "0.02548484387807548\n",
      "0.025808318983763456\n",
      "0.02550479117780924\n",
      "0.025511248968541622\n",
      "0.025546035962179303\n",
      "0.025513811968266964\n",
      "0.02552410401403904\n",
      "0.025496924063190818\n",
      "0.025469239801168442\n",
      "0.02587368478998542\n",
      "0.025680648162961006\n",
      "0.025600275956094265\n",
      "0.025476723909378052\n",
      "0.025489113060757518\n",
      "0.02547981613315642\n",
      "0.025512187974527478\n",
      "0.025469031184911728\n",
      "0.025918669998645782\n",
      "0.025546472053974867\n",
      "0.025528571801260114\n",
      "0.025687159039080143\n",
      "0.02567729097791016\n",
      "0.025498546892777085\n",
      "0.025744749000295997\n",
      "0.025554305175319314\n",
      "0.025812096893787384\n",
      "0.025578613160178065\n",
      "0.025489093968644738\n",
      "0.025455310940742493\n",
      "0.02551161195151508\n",
      "0.02553883707150817\n",
      "0.025639871135354042\n",
      "0.02554504480212927\n",
      "0.025835883105173707\n",
      "0.02570320898666978\n",
      "0.025498302886262536\n",
      "0.02553277602419257\n",
      "0.025517991976812482\n",
      "0.025521182920783758\n",
      "0.02552416012622416\n",
      "0.02555228304117918\n",
      "0.025890324031934142\n",
      "0.025665255961939692\n",
      "0.02554159890860319\n",
      "0.025577599881216884\n",
      "0.025552826933562756\n",
      "0.025507519021630287\n",
      "0.02550528012216091\n",
      "0.025602397974580526\n",
      "0.02583718509413302\n",
      "0.025646921014413238\n",
      "0.025806217920035124\n",
      "0.025541326962411404\n",
      "0.025567071046680212\n",
      "0.025505739962682128\n",
      "0.025479089003056288\n",
      "0.025596197927370667\n",
      "0.02579598408192396\n",
      "0.02559566986747086\n",
      "0.025558158988133073\n",
      "0.02549503999762237\n",
      "0.02552215219475329\n",
      "0.025486209895461798\n",
      "0.025523799005895853\n",
      "0.02558692405000329\n",
      "0.025763825979083776\n",
      "0.025558684952557087\n",
      "0.025481436867266893\n",
      "0.0255380691960454\n",
      "0.025480546057224274\n",
      "0.02558080805465579\n",
      "0.025513460859656334\n",
      "0.025613308185711503\n",
      "0.025840679882094264\n",
      "0.02560136583633721\n",
      "0.025540489004924893\n",
      "0.025482321856543422\n",
      "0.025514335837215185\n",
      "0.02550811693072319\n",
      "0.0255330060608685\n",
      "0.025480055017396808\n",
      "0.02588117984123528\n",
      "0.025483147939667106\n",
      "0.025494948960840702\n",
      "0.025594116188585758\n",
      "0.025505047058686614\n",
      "0.02554215001873672\n",
      "0.025496610207483172\n",
      "0.025519511196762323\n",
      "0.025807052152231336\n",
      "0.02558959787711501\n",
      "0.025433924049139023\n",
      "0.025498436065390706\n",
      "0.025531520135700703\n",
      "0.025550194084644318\n",
      "0.02558892499655485\n",
      "0.02550144400447607\n",
      "0.02585756895132363\n",
      "0.025590169010683894\n",
      "0.025474271038547158\n",
      "0.025541262002661824\n",
      "0.025590634904801846\n",
      "0.025524534983560443\n",
      "0.025521213887259364\n",
      "0.025537353940308094\n",
      "0.025828737998381257\n",
      "0.02559273806400597\n",
      "0.025680768070742488\n",
      "0.025519787101075053\n",
      "0.025705064879730344\n",
      "0.025545102078467607\n",
      "0.025529865873977542\n",
      "0.02553180605173111\n",
      "0.02583166491240263\n",
      "0.02550929202698171\n",
      "0.02552034705877304\n",
      "0.025521744042634964\n",
      "0.025452112080529332\n",
      "0.02556828805245459\n",
      "0.025544943986460567\n",
      "0.025528605096042156\n",
      "0.02595656900666654\n",
      "0.025564248207956553\n",
      "0.025470899185165763\n",
      "0.025549938902258873\n",
      "0.025558073073625565\n",
      "0.025542306946590543\n",
      "0.02555258199572563\n",
      "0.02551027201116085\n",
      "0.025756870862096548\n",
      "0.025586926145479083\n",
      "0.025532155064865947\n",
      "0.025565983029082417\n",
      "0.025528768077492714\n",
      "0.025550642982125282\n",
      "0.02555930707603693\n",
      "0.025506328092887998\n",
      "0.025907931150868535\n",
      "0.025535333203151822\n",
      "0.025545839918777347\n",
      "0.025497335009276867\n",
      "0.025547155179083347\n",
      "0.025592828867956996\n",
      "0.02554375701583922\n",
      "0.02554680616594851\n",
      "0.02591841504909098\n",
      "0.025498149916529655\n",
      "0.02556883287616074\n",
      "0.025576756801456213\n",
      "0.02556509990245104\n",
      "0.025572093902155757\n",
      "0.02557357307523489\n",
      "0.025538467103615403\n",
      "0.02587856096215546\n",
      "0.025471869157627225\n",
      "0.025572643848136067\n",
      "0.025512111140415072\n",
      "0.025510064093396068\n",
      "0.025493826949968934\n",
      "0.025559663074091077\n",
      "0.025536772096529603\n",
      "0.025914769154042006\n",
      "0.025510288076475263\n",
      "0.025484629906713963\n",
      "0.025675194105133414\n",
      "0.025674456963315606\n",
      "0.02555419411510229\n",
      "0.025535935070365667\n",
      "0.025548843201249838\n",
      "0.025863448157906532\n",
      "0.025508551159873605\n",
      "0.025486572878435254\n",
      "0.02551107620820403\n",
      "0.025627455906942487\n",
      "0.025482898112386465\n",
      "0.02549841208383441\n",
      "0.025552545906975865\n",
      "0.025824009208008647\n",
      "0.02554853493347764\n",
      "0.02549180807545781\n",
      "0.025496504036709666\n",
      "0.025503267999738455\n",
      "0.025527600897476077\n",
      "0.025612211786210537\n",
      "0.02552349492907524\n",
      "0.025952215073630214\n",
      "0.025647585978731513\n",
      "0.025625373935326934\n",
      "0.025540036847814918\n",
      "0.025486003141850233\n",
      "0.025524786906316876\n",
      "0.025500368792563677\n",
      "0.025517822010442615\n",
      "0.025944681139662862\n",
      "0.025509564904496074\n",
      "0.025490602012723684\n",
      "0.025503840995952487\n",
      "0.02550992090255022\n",
      "0.024945564102381468\n",
      "0.02551606181077659\n",
      "0.025520286988466978\n",
      "0.025338710052892566\n",
      "0.025477865943685174\n",
      "0.025497612077742815\n",
      "0.024413112085312605\n",
      "0.02551243407651782\n",
      "0.025631133001297712\n",
      "0.02553855092264712\n",
      "0.024330993881449103\n",
      "0.025896715931594372\n",
      "0.025486631086096168\n",
      "0.024437157902866602\n",
      "0.0254733229521662\n",
      "0.024647143902257085\n",
      "0.02552025206387043\n",
      "0.02554572792723775\n",
      "0.025495404144749045\n",
      "0.02721261791884899\n",
      "0.0267847771756351\n",
      "0.02425781195051968\n",
      "0.025508537888526917\n",
      "0.02563494094647467\n",
      "0.025600641034543514\n",
      "0.025739745004102588\n",
      "0.025610632030293345\n",
      "0.025771814864128828\n",
      "0.025556233013048768\n",
      "0.025497511960566044\n",
      "0.025479195173829794\n",
      "0.0254042181186378\n",
      "0.025498381815850735\n",
      "0.02556760306470096\n",
      "0.025551646947860718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02580784587189555\n",
      "0.025495446985587478\n",
      "0.025516226887702942\n",
      "0.025519188027828932\n",
      "0.02557788509875536\n",
      "0.02550031105056405\n",
      "0.025480580981820822\n",
      "0.025603678077459335\n",
      "0.025931284064427018\n",
      "0.025549670914188027\n",
      "0.025542321847751737\n",
      "0.02554355002939701\n",
      "0.02555293613113463\n",
      "0.025536941131576896\n",
      "0.025564586045220494\n",
      "0.025590162025764585\n",
      "0.025887528900057077\n",
      "0.025574947940185666\n",
      "0.025491873966529965\n",
      "0.02554924786090851\n",
      "0.025552073027938604\n",
      "0.025515676010400057\n",
      "0.025608599884435534\n",
      "0.025496691931039095\n",
      "0.025803142925724387\n",
      "0.025480807991698384\n",
      "0.02554035116918385\n",
      "0.025495920795947313\n",
      "0.02559947711415589\n",
      "0.02556603797711432\n",
      "0.025525072123855352\n",
      "0.025529479142278433\n",
      "0.025855398969724774\n",
      "0.02548466296866536\n",
      "0.0255084119271487\n",
      "0.025522310053929687\n",
      "0.025537445908412337\n",
      "0.02547762799076736\n",
      "0.025503565790131688\n",
      "0.02554540801793337\n",
      "0.02583305793814361\n",
      "0.025471266824752092\n",
      "0.025528546888381243\n",
      "0.02554445108398795\n",
      "0.025530786952003837\n",
      "0.02562378183938563\n",
      "0.02576540899462998\n",
      "0.0255910640116781\n",
      "0.02594078308902681\n",
      "0.025519666029140353\n",
      "0.02546059200540185\n",
      "0.025526126148179173\n",
      "0.025444159982725978\n",
      "0.025499992072582245\n",
      "0.025536508997902274\n",
      "0.025592343881726265\n",
      "0.025834996020421386\n",
      "0.02558252401649952\n",
      "0.02558116614818573\n",
      "0.02547456114552915\n",
      "0.025558438152074814\n",
      "0.025580090936273336\n",
      "0.02551814797334373\n",
      "0.02568567404523492\n",
      "0.025808576960116625\n",
      "0.02551205293275416\n",
      "0.02553678979165852\n",
      "0.025518002919852734\n",
      "0.025542313000187278\n",
      "0.02551131695508957\n",
      "0.025541767943650484\n",
      "0.02557940990664065\n",
      "0.025771779008209705\n",
      "0.025551907951012254\n",
      "0.02559387101791799\n",
      "0.025490195024758577\n",
      "0.02547880308702588\n",
      "0.02550621284171939\n",
      "0.025571302976459265\n",
      "0.025523688877001405\n",
      "0.025802555959671736\n",
      "0.02552122506313026\n",
      "0.025533128064125776\n",
      "0.025547209894284606\n",
      "0.02555997995659709\n",
      "0.0256044608540833\n",
      "0.025521377101540565\n",
      "0.025553534040227532\n",
      "0.02575898915529251\n",
      "0.025501929922029376\n",
      "0.02551155793480575\n",
      "0.0254863069858402\n",
      "0.025633214972913265\n",
      "0.025438155978918076\n",
      "0.025474627036601305\n",
      "0.025509410072118044\n",
      "0.025751581881195307\n",
      "0.02568975300528109\n",
      "0.025516211986541748\n",
      "0.02553156903013587\n",
      "0.025505962083116174\n",
      "0.025509047089144588\n",
      "0.02585464296862483\n",
      "0.025743355974555016\n",
      "0.025895135942846537\n",
      "0.02557200100272894\n",
      "0.025549945887178183\n",
      "0.025500928051769733\n",
      "0.02551893494091928\n",
      "0.02549954899586737\n",
      "0.025579825043678284\n",
      "0.025534182088449597\n",
      "0.02585384901612997\n",
      "0.025453479029238224\n",
      "0.02561275102198124\n",
      "0.025483675999566913\n",
      "0.02548483107239008\n",
      "0.02563137304969132\n",
      "0.025540933944284916\n",
      "0.025531717110425234\n",
      "0.02583696902729571\n",
      "0.025511681800708175\n",
      "0.025559404864907265\n",
      "0.0254745960701257\n",
      "0.02548752399161458\n",
      "0.025535922031849623\n",
      "0.02553700585849583\n",
      "0.02549049397930503\n",
      "0.025832241866737604\n",
      "0.0255293739028275\n",
      "0.02563715586438775\n",
      "0.02553567197173834\n",
      "0.025418672943487763\n",
      "0.025549083948135376\n",
      "0.025770523119717836\n",
      "0.025564306881278753\n",
      "0.025838772067800164\n",
      "0.02565254387445748\n",
      "0.025534912012517452\n",
      "0.02550351293757558\n",
      "0.025640116073191166\n",
      "0.025513199856504798\n",
      "0.02548110205680132\n",
      "0.025451801950111985\n",
      "0.025742952013388276\n",
      "0.02547374996356666\n",
      "0.025474634021520615\n",
      "0.0254806000739336\n",
      "0.02551578590646386\n",
      "0.02553906897082925\n",
      "0.02548435190692544\n",
      "0.025471715023741126\n",
      "0.025746533880010247\n",
      "0.02547324914485216\n",
      "0.025610723067075014\n",
      "0.025466963183134794\n",
      "0.02553380304016173\n",
      "0.025480577955022454\n",
      "0.026057144859805703\n",
      "0.02561016078107059\n",
      "0.025900153908878565\n",
      "0.02544851112179458\n",
      "0.025491076055914164\n",
      "0.02551116794347763\n",
      "0.02549208514392376\n",
      "0.025547480210661888\n",
      "0.025541267823427916\n",
      "0.02554664295166731\n",
      "0.025932431919500232\n",
      "0.025554507039487362\n",
      "0.02546356781385839\n",
      "0.025532307103276253\n",
      "0.025515947956591845\n",
      "0.025468443054705858\n",
      "0.025481058983132243\n",
      "0.025510625913739204\n",
      "0.02583770896308124\n",
      "0.025531987193971872\n",
      "0.025502986973151565\n",
      "0.025516269961372018\n",
      "0.025545615004375577\n",
      "0.025497217196971178\n",
      "0.025502795120701194\n",
      "0.025471695931628346\n",
      "0.025712711038067937\n",
      "0.025438770884647965\n",
      "0.025527565041556954\n",
      "0.02547057601623237\n",
      "0.025485669961199164\n",
      "0.02556267101317644\n",
      "0.02551461709663272\n",
      "0.02552188909612596\n",
      "0.025855593848973513\n",
      "0.025490036001428962\n",
      "0.02545863506384194\n",
      "0.025460093980655074\n",
      "0.025457710027694702\n",
      "0.025658075930550694\n",
      "0.025549151934683323\n",
      "0.02549795201048255\n",
      "0.02583514293655753\n",
      "0.02552514197304845\n",
      "0.025560426991432905\n",
      "0.02553712809458375\n",
      "0.025506359990686178\n",
      "0.025596945080906153\n",
      "0.02551708510145545\n",
      "0.025596254970878363\n",
      "0.025829672114923596\n",
      "0.025548017816618085\n",
      "0.02554516401141882\n",
      "0.025548831094056368\n",
      "0.02554319193586707\n",
      "0.025513796135783195\n",
      "0.025489388033747673\n",
      "0.025689748115837574\n",
      "0.025859301909804344\n",
      "0.025620626052841544\n",
      "0.02556032594293356\n",
      "0.0255031471606344\n",
      "0.02553900214843452\n",
      "0.0255584130063653\n",
      "0.025474586989730597\n",
      "0.025517943082377315\n",
      "0.02586315991356969\n",
      "0.02550185890868306\n",
      "0.025504204910248518\n",
      "0.025493497028946877\n",
      "0.025481840129941702\n",
      "0.025575964944437146\n",
      "0.025552073027938604\n",
      "0.02553659095428884\n",
      "0.025842580944299698\n",
      "0.025533769046887755\n",
      "0.02548978803679347\n",
      "0.025489254156127572\n",
      "0.025487663922831416\n",
      "0.02558990614488721\n",
      "0.025466276798397303\n",
      "0.025536551140248775\n",
      "0.025902305962517858\n",
      "0.02553208591416478\n",
      "0.02544786618091166\n",
      "0.02557080192491412\n",
      "0.02547538303770125\n",
      "0.025505695957690477\n",
      "0.025566532975062728\n",
      "0.025502254022285342\n",
      "0.02580529497936368\n",
      "0.025548516074195504\n",
      "0.025473826099187136\n",
      "0.025579607114195824\n",
      "0.025466941064223647\n",
      "0.025555020896717906\n",
      "0.02563349809497595\n",
      "0.025542899034917355\n",
      "0.025795577093958855\n",
      "0.02558215195313096\n",
      "0.025509638944640756\n",
      "0.02547701820731163\n",
      "0.02551174908876419\n",
      "0.02553961891680956\n",
      "0.02555719600059092\n",
      "0.025542180985212326\n",
      "0.025844521122053266\n",
      "0.025539326947182417\n",
      "0.025592393008992076\n",
      "0.025534062180668116\n",
      "0.025491264881566167\n",
      "0.02559326309710741\n",
      "0.025491427863016725\n",
      "0.02559113083407283\n",
      "0.026069425977766514\n",
      "0.025605530012398958\n",
      "0.02572595397941768\n",
      "0.02556368801742792\n",
      "0.025519311893731356\n",
      "0.02553726313635707\n",
      "0.025586667004972696\n",
      "0.025499489158391953\n",
      "0.02585315192118287\n",
      "0.025521473959088326\n",
      "0.025509421015158296\n",
      "0.02548145013861358\n",
      "0.02551664086058736\n",
      "0.025540262926369905\n",
      "0.025551554979756474\n",
      "0.025533969048410654\n",
      "0.025877338834106922\n",
      "0.025497816037386656\n",
      "0.02551106596365571\n",
      "0.025596546940505505\n",
      "0.025645660003647208\n",
      "0.02556200185790658\n",
      "0.0256263620685786\n",
      "0.025542679941281676\n",
      "0.025763876968994737\n",
      "0.025638754945248365\n",
      "0.025625266833230853\n",
      "0.02559893811121583\n",
      "0.025555385975167155\n",
      "0.02556260279379785\n",
      "0.025523481890559196\n",
      "0.025593287078663707\n",
      "0.025881957029923797\n",
      "0.025549967074766755\n",
      "0.025560064939782023\n",
      "0.025582316098734736\n",
      "0.025626236107200384\n",
      "0.025508695049211383\n",
      "0.025567963952198625\n",
      "0.025600715074688196\n",
      "0.025909027783200145\n",
      "0.02553269499912858\n",
      "0.025561901042237878\n",
      "0.025467202998697758\n",
      "0.02554175886325538\n",
      "0.025491931941360235\n",
      "0.02561700204387307\n",
      "0.025455383118242025\n",
      "0.025846763979643583\n",
      "0.025523012038320303\n",
      "0.02548547089099884\n",
      "0.02549937809817493\n",
      "0.025564897106960416\n",
      "0.025516730966046453\n",
      "0.025504684075713158\n",
      "0.025487626902759075\n",
      "0.025842907838523388\n",
      "0.025900501990690827\n",
      "0.02554253488779068\n",
      "0.025563571834936738\n",
      "0.02558383601717651\n",
      "0.025617678882554173\n",
      "0.025570844998583198\n",
      "0.025569034041836858\n",
      "0.025815578876063228\n",
      "0.02552530891261995\n",
      "0.025583212031051517\n",
      "0.02552612405270338\n",
      "0.02552111493423581\n",
      "0.025560570182278752\n",
      "0.02552228095009923\n",
      "0.02564820391125977\n",
      "0.02586649521254003\n",
      "0.025452518835663795\n",
      "0.02546587586402893\n",
      "0.025547202909365296\n",
      "0.02618542010895908\n",
      "0.026230581803247333\n",
      "0.02626488683745265\n",
      "0.02563364407978952\n",
      "0.02645529992878437\n",
      "0.026170549914240837\n",
      "0.026215407997369766\n",
      "0.025502611184492707\n",
      "0.026184200076386333\n",
      "0.02568889013491571\n",
      "0.0254886110778898\n",
      "0.025564417941495776\n",
      "0.02579639106988907\n",
      "0.027177838841453195\n",
      "0.02711838996037841\n",
      "0.025728032924234867\n",
      "0.025512924883514643\n",
      "0.025606022914871573\n",
      "0.025692750932648778\n",
      "0.02549571800045669\n",
      "0.025835579028353095\n",
      "0.025521944044157863\n",
      "0.025635930942371488\n",
      "0.025559040950611234\n",
      "0.02551287692040205\n",
      "0.02554104197770357\n",
      "0.025561292888596654\n",
      "0.025596332037821412\n",
      "0.025806814897805452\n",
      "0.02550173201598227\n",
      "0.025527614168822765\n",
      "0.025587704963982105\n",
      "0.025504320859909058\n",
      "0.02559491991996765\n",
      "0.025473151123151183\n",
      "0.025492405984550714\n",
      "0.025830541038885713\n",
      "0.02571654482744634\n",
      "0.025566054973751307\n",
      "0.02569269691593945\n",
      "0.02558854012750089\n",
      "0.025523365009576082\n",
      "0.025550226913765073\n",
      "0.025531989987939596\n",
      "0.025800929171964526\n",
      "0.025517066940665245\n",
      "0.02553334296680987\n",
      "0.025540099944919348\n",
      "0.02541611297056079\n",
      "0.02552806492894888\n",
      "0.025543865049257874\n",
      "0.025511244079098105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025839917128905654\n",
      "0.025545526994392276\n",
      "0.025578150991350412\n",
      "0.02560305898077786\n",
      "0.025544409872964025\n",
      "0.025559778092429042\n",
      "0.02561454800888896\n",
      "0.025569865945726633\n",
      "0.02586249401792884\n",
      "0.025538509944453835\n",
      "0.025537363020703197\n",
      "0.025657644029706717\n",
      "0.025608332129195333\n",
      "0.025628444040194154\n",
      "0.025515907909721136\n",
      "0.025503437034785748\n",
      "0.025773734087124467\n",
      "0.025560008827596903\n",
      "0.02555709588341415\n",
      "0.02562382398173213\n",
      "0.02547098882496357\n",
      "0.0255639951210469\n",
      "0.02551803016103804\n",
      "0.02561426186002791\n",
      "0.025900312000885606\n",
      "0.025483798934146762\n",
      "0.025511682964861393\n",
      "0.025476991897448897\n",
      "0.025498102884739637\n",
      "0.02552383905276656\n",
      "0.025590826058760285\n",
      "0.025512900901958346\n",
      "0.02577764797024429\n",
      "0.02561287395656109\n",
      "0.025576840853318572\n",
      "0.025521264178678393\n",
      "0.025603858986869454\n",
      "0.025546940974891186\n",
      "0.02552804397419095\n",
      "0.025523368967697024\n",
      "0.025772440945729613\n",
      "0.025539825204759836\n",
      "0.025832146871834993\n",
      "0.02554186014458537\n",
      "0.025518304901197553\n",
      "0.025510734878480434\n",
      "0.025539888069033623\n",
      "0.025563423056155443\n",
      "0.025797994108870625\n",
      "0.02549482393078506\n",
      "0.025696032913401723\n",
      "0.025575725128874183\n",
      "0.025523848831653595\n",
      "0.025485822930932045\n",
      "0.025519234128296375\n",
      "0.02552787004970014\n",
      "0.025856378953903913\n",
      "0.025577894877642393\n",
      "0.025547576136887074\n",
      "0.02550510596483946\n",
      "0.02553757489658892\n",
      "0.025613290956243873\n",
      "0.02551830094307661\n",
      "0.025488676968961954\n",
      "0.02576332283206284\n",
      "0.025484807789325714\n",
      "0.025502979988232255\n",
      "0.025501169031485915\n",
      "0.025595013983547688\n",
      "0.02549821208231151\n",
      "0.025568866869434714\n",
      "0.025569591904059052\n",
      "0.02578386990353465\n",
      "0.025495710084214807\n",
      "0.02559130499139428\n",
      "0.025526437908411026\n",
      "0.025515265995636582\n",
      "0.025442396057769656\n",
      "0.02549707586877048\n",
      "0.025512779131531715\n",
      "0.025811498053371906\n",
      "0.025537786073982716\n",
      "0.025570831960067153\n",
      "0.025489973835647106\n",
      "0.025488399202004075\n",
      "0.025548073928803205\n",
      "0.025531487073749304\n",
      "0.025515098124742508\n",
      "0.025895701022818685\n",
      "0.025641876040026546\n",
      "0.025549137964844704\n",
      "0.025469370186328888\n",
      "0.025501458905637264\n",
      "0.025542083894833922\n",
      "0.025539455004036427\n",
      "0.025520626921206713\n",
      "0.025757503928616643\n",
      "0.02558033703826368\n",
      "0.025662765139713883\n",
      "0.02559682005085051\n",
      "0.025569492019712925\n",
      "0.02548407786525786\n",
      "0.025516821071505547\n",
      "0.025489063002169132\n",
      "0.025835055159404874\n",
      "0.025684682186692953\n",
      "0.025772948982194066\n",
      "0.0256100760307163\n",
      "0.025532283121719956\n",
      "0.02553272619843483\n",
      "0.025536194909363985\n",
      "0.02565103000961244\n",
      "0.025953116826713085\n",
      "0.025576284853741527\n",
      "0.025600915076211095\n",
      "0.025483581935986876\n",
      "0.025556987849995494\n",
      "0.02565399999730289\n",
      "0.025511442916467786\n",
      "0.025754034984856844\n",
      "0.02579466998577118\n",
      "0.025516622001305223\n",
      "0.025587050011381507\n",
      "0.025536620058119297\n",
      "0.025553573155775666\n",
      "0.025527494959533215\n",
      "0.025554110994562507\n",
      "0.02550110616721213\n",
      "0.025827785022556782\n",
      "0.025508558144792914\n",
      "0.025566542986780405\n",
      "0.025523510994389653\n",
      "0.025510483188554645\n",
      "0.02560863201506436\n",
      "0.02552512986585498\n",
      "0.02550016506575048\n",
      "0.02593634487129748\n",
      "0.025561070069670677\n",
      "0.025520266965031624\n",
      "0.025563186034560204\n",
      "0.025507776997983456\n",
      "0.025526766199618578\n",
      "0.025556195061653852\n",
      "0.025502939010038972\n",
      "0.02579204412177205\n",
      "0.02574776508845389\n",
      "0.025527880992740393\n",
      "0.02551037701778114\n",
      "0.02549059083685279\n",
      "0.02550210594199598\n",
      "0.025511470856145024\n",
      "0.025518614100292325\n",
      "0.025847314158454537\n",
      "0.025576845044270158\n",
      "0.025498562958091497\n",
      "0.025501764146611094\n",
      "0.025659472914412618\n",
      "0.025645192014053464\n",
      "0.02557475515641272\n",
      "0.025512342108413577\n",
      "0.025834328029304743\n",
      "0.02554964693263173\n",
      "0.02551144501194358\n",
      "0.025560032110661268\n",
      "0.025493501918390393\n",
      "0.02547370013780892\n",
      "0.025511607062071562\n",
      "0.025493596913293004\n",
      "0.026007082080468535\n",
      "0.025626175105571747\n",
      "0.025563908042386174\n",
      "0.025479183066636324\n",
      "0.025514857843518257\n",
      "0.025497321039438248\n",
      "0.025563751813024282\n",
      "0.025543554918840528\n",
      "0.025766576873138547\n",
      "0.02561025507748127\n",
      "0.025540477829053998\n",
      "0.025539547903463244\n",
      "0.02555079315789044\n",
      "0.02556282188743353\n",
      "0.02552137104794383\n",
      "0.02564547793008387\n",
      "0.025776774156838655\n",
      "0.0254995699506253\n",
      "0.025524221127852798\n",
      "0.025488566141575575\n",
      "0.025561556918546557\n",
      "0.02552118804305792\n",
      "0.025586705189198256\n",
      "0.02550898096524179\n",
      "0.025852878112345934\n",
      "0.02581907412968576\n",
      "0.025550791062414646\n",
      "0.025508587015792727\n",
      "0.025472199078649282\n",
      "0.025571369100362062\n",
      "0.02555034589022398\n",
      "0.025528924074023962\n",
      "0.02579623111523688\n",
      "0.025630631018429995\n",
      "0.02560481382533908\n",
      "0.02557354886084795\n",
      "0.0255616488866508\n",
      "0.025541265960782766\n",
      "0.025519379880279303\n",
      "0.02561842813156545\n",
      "0.02587525499984622\n",
      "0.025603770976886153\n",
      "0.025538646150380373\n",
      "0.02564755897037685\n",
      "0.02569550508633256\n",
      "0.025681503117084503\n",
      "0.02549095219001174\n",
      "0.02553837699815631\n",
      "0.02587394998408854\n",
      "0.025626322021707892\n",
      "0.025518330978229642\n",
      "0.02548665413632989\n",
      "0.025508598191663623\n",
      "0.025492700980976224\n",
      "0.02550139813683927\n",
      "0.025593038881197572\n",
      "0.025855316082015634\n",
      "0.025585358031094074\n",
      "0.02543560299091041\n",
      "0.025499308947473764\n",
      "0.025519453920423985\n",
      "0.025648012990131974\n",
      "0.0256445009727031\n",
      "0.025529175996780396\n",
      "0.025875893188640475\n",
      "0.02557979105040431\n",
      "0.025489550083875656\n",
      "0.02548704412765801\n",
      "0.02549287606962025\n",
      "0.02550903195515275\n",
      "0.025566583964973688\n",
      "0.02560522104613483\n",
      "0.025881993817165494\n",
      "0.025514413136988878\n",
      "0.025475980015471578\n",
      "0.025463398080319166\n",
      "0.025490686995908618\n",
      "0.025469624903053045\n",
      "0.02560955611988902\n",
      "0.02545173210091889\n",
      "0.025924240006133914\n",
      "0.02555827610194683\n",
      "0.0254452598746866\n",
      "0.02554506598971784\n",
      "0.025533545995131135\n",
      "0.025504373013973236\n",
      "0.025485209887847304\n",
      "0.025519936811178923\n",
      "0.025885391980409622\n",
      "0.025538546033203602\n",
      "0.025532576953992248\n",
      "0.025579896057024598\n",
      "0.02545625390484929\n",
      "0.025578088825568557\n",
      "0.025483283912763\n",
      "0.02558815898373723\n",
      "0.02586639695800841\n",
      "0.025493387831375003\n",
      "0.025503657991066575\n",
      "0.025510507868602872\n",
      "0.025575696025043726\n",
      "0.025743730133399367\n",
      "0.02551709092222154\n",
      "0.025715510826557875\n",
      "0.02581788320094347\n",
      "0.02557629393413663\n",
      "0.025510979117825627\n",
      "0.025505756959319115\n",
      "0.025604954920709133\n",
      "0.02555073401890695\n",
      "0.025471878005191684\n",
      "0.025455713039264083\n",
      "0.02587016997858882\n",
      "0.025505875935778022\n",
      "0.025663022883236408\n",
      "0.02551909489557147\n",
      "0.02549156593158841\n",
      "0.025586466072127223\n",
      "0.02548553002998233\n",
      "0.025499739916995168\n",
      "0.025833310093730688\n",
      "0.025488150073215365\n",
      "0.025527447229251266\n",
      "0.025516018038615584\n",
      "0.025531229795888066\n",
      "0.025518845999613404\n",
      "0.02551518799737096\n",
      "0.025528327096253633\n",
      "0.025820338865742087\n",
      "0.025579713052138686\n",
      "0.02555248001590371\n",
      "0.025527595076709986\n",
      "0.02553017414174974\n",
      "0.025645391084253788\n",
      "0.02558168093673885\n",
      "0.025572218000888824\n",
      "0.02585021103732288\n",
      "0.02554063592106104\n",
      "0.025586470030248165\n",
      "0.025491044856607914\n",
      "0.025506347185000777\n",
      "0.025482576107606292\n",
      "0.02548677008599043\n",
      "0.02547899098135531\n",
      "0.025825308868661523\n",
      "0.025509136030450463\n",
      "0.02555331285111606\n",
      "0.025507475016638637\n",
      "0.025505241937935352\n",
      "0.025524491909891367\n",
      "0.025491794105619192\n",
      "0.02548248297534883\n",
      "0.025918567087501287\n",
      "0.025539451045915484\n",
      "0.02548758708871901\n",
      "0.0255136601626873\n",
      "0.025549995945766568\n",
      "0.02557926089502871\n",
      "0.02556766802445054\n",
      "0.02570330211892724\n",
      "0.025918720057234168\n",
      "0.02555881906300783\n",
      "0.025596375111490488\n",
      "0.02550462889485061\n",
      "0.025677175959572196\n",
      "0.025513472966849804\n",
      "0.025490522850304842\n",
      "0.025563142029568553\n",
      "0.025834023021161556\n",
      "0.025621093111112714\n",
      "0.027615325059741735\n",
      "0.02609942597337067\n",
      "0.025469057960435748\n",
      "0.02553324308246374\n",
      "0.025576400803402066\n",
      "0.025548703968524933\n",
      "0.025767663959413767\n",
      "0.02551414491608739\n",
      "0.02571706986054778\n",
      "0.025556746870279312\n",
      "0.02546734595671296\n",
      "0.025557084009051323\n",
      "0.025504290824756026\n",
      "0.02559128706343472\n",
      "0.025929307099431753\n",
      "0.025558967841789126\n",
      "0.025480123003944755\n",
      "0.025512155843898654\n",
      "0.025456096976995468\n",
      "0.0255692380014807\n",
      "0.02551134186796844\n",
      "0.02550885104574263\n",
      "0.025746801868081093\n",
      "0.02549157594330609\n",
      "0.025470958091318607\n",
      "0.02555920695886016\n",
      "0.02555616502650082\n",
      "0.025516166118904948\n",
      "0.025505347875878215\n",
      "0.025509498082101345\n",
      "0.02580352290533483\n",
      "0.025500878924503922\n",
      "0.025543289026245475\n",
      "0.02547406405210495\n",
      "0.025523998076096177\n",
      "0.025515194050967693\n",
      "0.025477163027971983\n",
      "0.02551521989516914\n",
      "0.025853116996586323\n",
      "0.025434058159589767\n",
      "0.025451028952375054\n",
      "0.025457304902374744\n",
      "0.02545314095914364\n",
      "0.025607261108234525\n",
      "0.025505197932943702\n",
      "0.025816640118137002\n",
      "0.02597750606946647\n",
      "0.02545807301066816\n",
      "0.025485296035185456\n",
      "0.025440796045586467\n",
      "0.02547472482547164\n",
      "0.025501458905637264\n",
      "0.025639674859121442\n",
      "0.025520586874336004\n",
      "0.0258017061278224\n",
      "0.025487476028501987\n",
      "0.025462626945227385\n",
      "0.025598757900297642\n",
      "0.025402880972251296\n",
      "0.025493266992270947\n",
      "0.025512153981253505\n",
      "0.02550684497691691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025840235175564885\n",
      "0.02566757006570697\n",
      "0.025480816839262843\n",
      "0.025509702041745186\n",
      "0.025547228986397386\n",
      "0.025524099823087454\n",
      "0.025504854042083025\n",
      "0.025566315045580268\n",
      "0.025960723869502544\n",
      "0.02560955611988902\n",
      "0.025481659919023514\n",
      "0.025516831083223224\n",
      "0.025466039078310132\n",
      "0.025529128033667803\n",
      "0.025424971943721175\n",
      "0.025475909002125263\n",
      "0.025782108074054122\n",
      "0.025493680965155363\n",
      "0.025462619960308075\n",
      "0.025444598868489265\n",
      "0.025477417977526784\n",
      "0.02547144005075097\n",
      "0.025409796042367816\n",
      "0.025555690051987767\n",
      "0.025902464985847473\n",
      "0.02560828998684883\n",
      "0.025490960106253624\n",
      "0.025506868958473206\n",
      "0.025496081914752722\n",
      "0.025436467956751585\n",
      "0.025532724102959037\n",
      "0.025441386038437486\n",
      "0.025793121894821525\n",
      "0.025660524843260646\n",
      "0.025460134958848357\n",
      "0.02548612211830914\n",
      "0.025477810064330697\n",
      "0.025508856866508722\n",
      "0.025555489817634225\n",
      "0.025464925915002823\n",
      "0.025880313012748957\n",
      "0.025644231820479035\n",
      "0.0254945729393512\n",
      "0.025469314074143767\n",
      "0.02548944903537631\n",
      "0.025550169171765447\n",
      "0.025462611112743616\n",
      "0.025545286014676094\n",
      "0.02578721195459366\n",
      "0.025454475078731775\n",
      "0.025456496980041265\n",
      "0.02544454694725573\n",
      "0.025524676078930497\n",
      "0.025476646842435002\n",
      "0.025523562915623188\n",
      "0.025457364972680807\n",
      "0.02580616297200322\n",
      "0.02553133782930672\n",
      "0.025469935964792967\n",
      "0.025592732010409236\n",
      "0.02542422292754054\n",
      "0.025473920162767172\n",
      "0.02558434591628611\n",
      "0.025646416936069727\n",
      "0.02579850284382701\n",
      "0.02554475492797792\n",
      "0.025593216065317392\n",
      "0.0254462999291718\n",
      "0.025444722967222333\n",
      "0.025465501938015223\n",
      "0.025574637111276388\n",
      "0.02554245200008154\n",
      "0.025795608991757035\n",
      "0.025435579009354115\n",
      "0.02545268298126757\n",
      "0.025578739121556282\n",
      "0.025478364899754524\n",
      "0.02556991088204086\n",
      "0.02559960586950183\n",
      "0.025462214136496186\n",
      "0.0257857961114496\n",
      "0.025514356093481183\n",
      "0.02561644301749766\n",
      "0.025664438027888536\n",
      "0.025592575082555413\n",
      "0.025630973046645522\n",
      "0.02556536509655416\n",
      "0.025559603003785014\n",
      "0.02579500712454319\n",
      "0.025613847188651562\n",
      "0.025477783055976033\n",
      "0.02545437403023243\n",
      "0.025473288958892226\n",
      "0.025503707118332386\n",
      "0.02562802191823721\n",
      "0.025484719080850482\n",
      "0.025893745943903923\n",
      "0.025744357844814658\n",
      "0.025666194036602974\n",
      "0.025564372073858976\n",
      "0.025508692022413015\n",
      "0.02564427093602717\n",
      "0.02570762299001217\n",
      "0.025624147849157453\n",
      "0.025914983125403523\n",
      "0.025562061928212643\n",
      "0.025589581113308668\n",
      "0.025618966203182936\n",
      "0.025701277889311314\n",
      "0.025469577172771096\n",
      "0.02556218602694571\n",
      "0.025492019951343536\n",
      "0.0257760570384562\n",
      "0.025631088996306062\n",
      "0.02552348398603499\n",
      "0.025449570966884494\n",
      "0.02544487197883427\n",
      "0.025513970060274005\n",
      "0.025575021049007773\n",
      "0.025521017145365477\n",
      "0.02580658788792789\n",
      "0.02576004504226148\n",
      "0.02562552480958402\n",
      "0.025562376948073506\n",
      "0.025522859068587422\n",
      "0.025501851923763752\n",
      "0.025569832185283303\n",
      "0.025551284896209836\n",
      "0.025760178919881582\n",
      "0.02552195219323039\n",
      "0.025526697048917413\n",
      "0.02549314801581204\n",
      "0.02560815098695457\n",
      "0.025584063958376646\n",
      "0.025527923833578825\n",
      "0.02552301692776382\n",
      "0.025803467025980353\n",
      "0.025545555166900158\n",
      "0.025507136015221477\n",
      "0.02544423402287066\n",
      "0.025556035107001662\n",
      "0.025794146116822958\n",
      "0.025624025147408247\n",
      "0.02550060418434441\n",
      "0.025814137887209654\n",
      "0.025557110086083412\n",
      "0.025649111019447446\n",
      "0.025481699034571648\n",
      "0.02550319512374699\n",
      "0.025579429930076003\n",
      "0.02555522508919239\n",
      "0.02556900097988546\n",
      "0.025780100841075182\n",
      "0.025760154938325286\n",
      "0.02560492604970932\n",
      "0.02565154805779457\n",
      "0.025635111145675182\n",
      "0.02550707687623799\n",
      "0.025568709010258317\n",
      "0.025569841964170337\n",
      "0.025872141122817993\n",
      "0.02548414492048323\n",
      "0.025477714836597443\n",
      "0.02554862597025931\n",
      "0.025453853886574507\n",
      "0.025539703899994493\n",
      "0.025476014940068126\n",
      "0.02547824988141656\n",
      "0.02575723989866674\n",
      "0.02548264106735587\n",
      "0.025603414047509432\n",
      "0.025462455116212368\n",
      "0.02553815604187548\n",
      "0.025516391033306718\n",
      "0.02548467298038304\n",
      "0.025490398984402418\n",
      "0.02576285693794489\n",
      "0.02545601991005242\n",
      "0.02556677209213376\n",
      "0.02555026300251484\n",
      "0.025634537916630507\n",
      "0.025572997983545065\n",
      "0.025420957012102008\n",
      "0.02553797815926373\n",
      "0.02591136796399951\n",
      "0.02548567997291684\n",
      "0.02546859602443874\n",
      "0.025497321039438248\n",
      "0.025469759944826365\n",
      "0.02559838304296136\n",
      "0.0254977960139513\n",
      "0.025565126910805702\n",
      "0.025784923927858472\n",
      "0.025472359033301473\n",
      "0.02548069995827973\n",
      "0.025474063819274306\n",
      "0.02542777801863849\n",
      "0.02567956503480673\n",
      "0.025613144040107727\n",
      "0.025646633934229612\n",
      "0.025825502816587687\n",
      "0.025530938990414143\n",
      "0.025615349877625704\n",
      "0.025525487959384918\n",
      "0.025582444854080677\n",
      "0.025624817004427314\n",
      "0.02554325805976987\n",
      "0.02562471805140376\n",
      "0.025822053896263242\n",
      "0.025517284171655774\n",
      "0.025827242992818356\n",
      "0.025592580903321505\n",
      "0.025532453088089824\n",
      "0.0256090578623116\n",
      "0.025547530967742205\n",
      "0.02554481802508235\n",
      "0.025891627883538604\n",
      "0.025582640198990703\n",
      "0.02552782418206334\n",
      "0.025627037044614553\n",
      "0.025686295004561543\n",
      "0.025666687870398164\n",
      "0.025606172857806087\n",
      "0.025515189161524177\n",
      "0.025890728924423456\n",
      "0.02555667795240879\n",
      "0.02553609712049365\n",
      "0.025512278778478503\n",
      "0.025488073006272316\n",
      "0.025598668958991766\n",
      "0.025528113823384047\n",
      "0.025417356053367257\n",
      "0.02583285397849977\n",
      "0.025463517988100648\n",
      "0.025609055999666452\n",
      "0.025544615928083658\n",
      "0.025610001990571618\n",
      "0.025510219857096672\n",
      "0.02561988914385438\n",
      "0.02552869380451739\n",
      "0.025789484148845077\n",
      "0.025613314006477594\n",
      "0.025584771065041423\n",
      "0.025507136015221477\n",
      "0.025552382925525308\n",
      "0.025578311877325177\n",
      "0.025482153985649347\n",
      "0.025548014091327786\n",
      "0.025880693923681974\n",
      "0.02553264913149178\n",
      "0.025547988014295697\n",
      "0.02550966595299542\n",
      "0.025614365004003048\n",
      "0.025570525089278817\n",
      "0.025472308974713087\n",
      "0.025548753095790744\n",
      "0.025767133804038167\n",
      "0.025488340063020587\n",
      "0.025541346054524183\n",
      "0.025668736081570387\n",
      "0.025544411968439817\n",
      "0.02550143003463745\n",
      "0.025512611027806997\n",
      "0.025502042146399617\n",
      "0.025888680946081877\n",
      "0.02550459885969758\n",
      "0.025506027974188328\n",
      "0.025812871055677533\n",
      "0.025568894809111953\n",
      "0.025642843218520284\n",
      "0.025507485028356314\n",
      "0.025570655008777976\n",
      "0.025829067919403315\n",
      "0.02555510215461254\n",
      "0.025526212062686682\n",
      "0.025576740968972445\n",
      "0.02557841595262289\n",
      "0.025523990858346224\n",
      "0.025910306023433805\n",
      "0.02550177089869976\n",
      "0.025827093981206417\n",
      "0.025539332069456577\n",
      "0.025481723016127944\n",
      "0.025527183199301362\n",
      "0.02556296600960195\n",
      "0.02551388298161328\n",
      "0.025495381094515324\n",
      "0.02552654594182968\n",
      "0.02576754498295486\n",
      "0.025509524159133434\n",
      "0.025506068021059036\n",
      "0.025507880141958594\n",
      "0.025697811041027308\n",
      "0.02552922791801393\n",
      "0.02550289500504732\n",
      "0.025487584061920643\n",
      "0.025727259926497936\n",
      "0.025787954917177558\n",
      "0.025526746874675155\n",
      "0.025654833996668458\n",
      "0.0256517028901726\n",
      "0.02551423409022391\n",
      "0.025557409040629864\n",
      "0.025542966090142727\n",
      "0.025899740168824792\n",
      "0.025507075944915414\n",
      "0.025459143798798323\n",
      "0.025505939964205027\n",
      "0.025504158111289144\n",
      "0.025509458035230637\n",
      "0.025538397021591663\n",
      "0.025551262078806758\n",
      "0.025843399111181498\n",
      "0.025525362929329276\n",
      "0.025519880931824446\n",
      "0.026638747891411185\n",
      "0.02686732984147966\n",
      "0.025532024912536144\n",
      "0.02550562215037644\n",
      "0.02552208793349564\n",
      "0.025742085184901953\n",
      "0.025535242864862084\n",
      "0.025503257988020778\n",
      "0.025611805962398648\n",
      "0.025633344892412424\n",
      "0.025663058971986175\n",
      "0.025559303117915988\n",
      "0.02548750815913081\n",
      "0.025787909980863333\n",
      "0.025568983983248472\n",
      "0.025554270017892122\n",
      "0.025545153999701142\n",
      "0.025504346005618572\n",
      "0.025480546057224274\n",
      "0.025523592019453645\n",
      "0.025600071996450424\n",
      "0.02579783508554101\n",
      "0.025535227032378316\n",
      "0.02546004601754248\n",
      "0.02557307993993163\n",
      "0.025490574073046446\n",
      "0.025459073949605227\n",
      "0.025471170898526907\n",
      "0.025513866916298866\n",
      "0.02574405586346984\n",
      "0.025516666006296873\n",
      "0.02547985198907554\n",
      "0.02554203919135034\n",
      "0.025571472942829132\n",
      "0.02555499109439552\n",
      "0.025501545052975416\n",
      "0.025462480960413814\n",
      "0.0257776309736073\n",
      "0.025496460031718016\n",
      "0.02552195405587554\n",
      "0.025524772005155683\n",
      "0.025542456889525056\n",
      "0.02548196306452155\n",
      "0.025561410002410412\n",
      "0.025518669048324227\n",
      "0.025821197079494596\n",
      "0.02551425388082862\n",
      "0.025481668999418616\n",
      "0.025537710869684815\n",
      "0.025461246026679873\n",
      "0.02553021302446723\n",
      "0.025488144950941205\n",
      "0.0255298160482198\n",
      "0.025902854977175593\n",
      "0.025524968979880214\n",
      "0.025491117034107447\n",
      "0.025567834032699466\n",
      "0.02554329694248736\n",
      "0.025522246956825256\n",
      "0.02546668006107211\n",
      "0.025518746813759208\n",
      "0.02580359997227788\n",
      "0.02559872600249946\n",
      "0.02546755177900195\n",
      "0.025526375975459814\n",
      "0.025664800079539418\n",
      "0.025670771952718496\n",
      "0.02556899399496615\n",
      "0.02545263897627592\n",
      "0.02576840785332024\n",
      "0.025626248912885785\n",
      "0.025467189960181713\n",
      "0.025611103978008032\n",
      "0.025502166943624616\n",
      "0.02558599808253348\n",
      "0.025495946872979403\n",
      "0.025514937937259674\n",
      "0.025797407841309905\n",
      "0.025519713992252946\n",
      "0.025605949107557535\n",
      "0.02562334993854165\n",
      "0.02555563999339938\n",
      "0.025568471057340503\n",
      "0.025542702060192823\n",
      "0.025575932813808322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025805603014305234\n",
      "0.025485931895673275\n",
      "0.025526064913719893\n",
      "0.02557901805266738\n",
      "0.0255898789037019\n",
      "0.02560848300345242\n",
      "0.02552974713034928\n",
      "0.025541245006024837\n",
      "0.025830192025750875\n",
      "0.025489465100690722\n",
      "0.025565637974068522\n",
      "0.025536288041621447\n",
      "0.025495567824691534\n",
      "0.025505024939775467\n",
      "0.025500936899334192\n",
      "0.025586588075384498\n",
      "0.025859646033495665\n",
      "0.02550764987245202\n",
      "0.025483171921223402\n",
      "0.025557447923347354\n",
      "0.025470318971201777\n",
      "0.025547978933900595\n",
      "0.02551734517328441\n",
      "0.02550777606666088\n",
      "0.025824252981692553\n",
      "0.02561183786019683\n",
      "0.02547611203044653\n",
      "0.025565461022779346\n",
      "0.026715954998508096\n",
      "0.02559312805533409\n",
      "0.025484432000666857\n",
      "0.02549435803666711\n",
      "0.02580952807329595\n",
      "0.025475610978901386\n",
      "0.02567400294356048\n",
      "0.02549500297755003\n",
      "0.02551609603688121\n",
      "0.025715814903378487\n",
      "0.025590881006792188\n",
      "0.025653421878814697\n",
      "0.02577395411208272\n",
      "0.025483357021585107\n",
      "0.025631294818595052\n",
      "0.025568574899807572\n",
      "0.025498441886156797\n",
      "0.025470247957855463\n",
      "0.025433937087655067\n",
      "0.025473210029304028\n",
      "0.025925728026777506\n",
      "0.025490442058071494\n",
      "0.025504515040665865\n",
      "0.025496714049950242\n",
      "0.02551496890373528\n",
      "0.02565635507926345\n",
      "0.025509177008643746\n",
      "0.025511240120977163\n",
      "0.025797569891437888\n",
      "0.025533716892823577\n",
      "0.025548781035467982\n",
      "0.02558136684820056\n",
      "0.025687481043860316\n",
      "0.025585477938875556\n",
      "0.025556018110364676\n",
      "0.025447939056903124\n",
      "0.02577834390103817\n",
      "0.02561016590334475\n",
      "0.025557355023920536\n",
      "0.02556199999526143\n",
      "0.025629326002672315\n",
      "0.02550099603831768\n",
      "0.025503252865746617\n",
      "0.02556695998646319\n",
      "0.02576513309031725\n",
      "0.025501327123492956\n",
      "0.025618455838412046\n",
      "0.02544816886074841\n",
      "0.025503722950816154\n",
      "0.02546050399541855\n",
      "0.025477370945736766\n",
      "0.025496776215732098\n",
      "0.025846839183941483\n",
      "0.025602474110201\n",
      "0.025574255967512727\n",
      "0.025507332989946008\n",
      "0.025498877046629786\n",
      "0.02548669697716832\n",
      "0.025625029113143682\n",
      "0.0254515390843153\n",
      "0.02580731105990708\n",
      "0.025540183996781707\n",
      "0.02563779498450458\n",
      "0.025436918949708343\n",
      "0.025534437969326973\n",
      "0.02550661191344261\n",
      "0.02574607590213418\n",
      "0.02558849914930761\n",
      "0.025804475182667375\n",
      "0.025518067181110382\n",
      "0.02574260998517275\n",
      "0.025693967938423157\n",
      "0.025608557043597102\n",
      "0.025486842961981893\n",
      "0.02547190198674798\n",
      "0.02555417502298951\n",
      "0.025794029934331775\n",
      "0.025519161019474268\n",
      "0.025558226043358445\n",
      "0.025490990839898586\n",
      "0.025542427087202668\n",
      "0.025628503877669573\n",
      "0.02551400987431407\n",
      "0.025504808174446225\n",
      "0.02579685812816024\n",
      "0.025562592083588243\n",
      "0.025580344023182988\n",
      "0.025515962159261107\n",
      "0.02558132610283792\n",
      "0.025641884189099073\n",
      "0.025569613091647625\n",
      "0.025588672142475843\n",
      "0.02582779503427446\n",
      "0.025537780951708555\n",
      "0.02560184197500348\n",
      "0.025611968943849206\n",
      "0.025463346857577562\n",
      "0.025457628071308136\n",
      "0.025497956899926066\n",
      "0.025526390876621008\n",
      "0.02584159909747541\n",
      "0.02559670014306903\n",
      "0.02557427203282714\n",
      "0.025479652918875217\n",
      "0.02543277689255774\n",
      "0.025448875967413187\n",
      "0.025471864035353065\n",
      "0.025453201960772276\n",
      "0.025777267990633845\n",
      "0.02562788687646389\n",
      "0.02564246882684529\n",
      "0.025531794177368283\n",
      "0.02555439481511712\n",
      "0.02553340489976108\n",
      "0.02556998818181455\n",
      "0.025456851813942194\n",
      "0.025739046977832913\n",
      "0.025566344149410725\n",
      "0.02546328608877957\n",
      "0.025581615045666695\n",
      "0.02549261087551713\n",
      "0.025465778075158596\n",
      "0.025521305156871676\n",
      "0.025778307113796473\n",
      "0.025835933163762093\n",
      "0.025658003985881805\n",
      "0.025490902829915285\n",
      "0.02562855905853212\n",
      "0.025502868928015232\n",
      "0.025528742000460625\n",
      "0.025501934112980962\n",
      "0.02544128498993814\n",
      "0.025822303956374526\n",
      "0.02567950915545225\n",
      "0.02551422407850623\n",
      "0.02550530107691884\n",
      "0.02545006899163127\n",
      "0.02550147706642747\n",
      "0.025542159099131823\n",
      "0.025447852909564972\n",
      "0.025767273036763072\n",
      "0.02555782813578844\n",
      "0.025512649910524487\n",
      "0.02547611901536584\n",
      "0.02547878515906632\n",
      "0.02548123593442142\n",
      "0.02552854991517961\n",
      "0.02548638009466231\n",
      "0.025723156984895468\n",
      "0.025539705995470285\n",
      "0.025504186050966382\n",
      "0.025419159093871713\n",
      "0.025518914917483926\n",
      "0.025418170960620046\n",
      "0.025490107014775276\n",
      "0.025463385973125696\n",
      "0.02572257607243955\n",
      "0.025546526070684195\n",
      "0.02545505785383284\n",
      "0.025453859008848667\n",
      "0.025549742160364985\n",
      "0.025450305081903934\n",
      "0.025447955122217536\n",
      "0.02549753407947719\n",
      "0.025704893050715327\n",
      "0.025588225107640028\n",
      "0.02556596603244543\n",
      "0.02545453398488462\n",
      "0.025461247889325023\n",
      "0.025517913047224283\n",
      "0.025529410922899842\n",
      "0.025608355877920985\n",
      "0.02579488791525364\n",
      "0.025582946836948395\n",
      "0.025513526052236557\n",
      "0.02547824988141656\n",
      "0.025649237912148237\n",
      "0.02557276701554656\n",
      "0.025481598917394876\n",
      "0.025496209040284157\n",
      "0.026049922918900847\n",
      "0.025584775023162365\n",
      "0.025516533059999347\n",
      "0.0255779349245131\n",
      "0.02558629005216062\n",
      "0.025598396081477404\n",
      "0.025515184039250016\n",
      "0.025543736992403865\n",
      "0.025884535163640976\n",
      "0.02558662393130362\n",
      "0.025464432081207633\n",
      "0.025498335948213935\n",
      "0.025527779944241047\n",
      "0.025564946001395583\n",
      "0.025607953080907464\n",
      "0.025594555074349046\n",
      "0.025804087985306978\n",
      "0.025527101010084152\n",
      "0.025458910036832094\n",
      "0.025550871156156063\n",
      "0.025476997951045632\n",
      "0.025505549041554332\n",
      "0.025455327006056905\n",
      "0.025599671062082052\n",
      "0.02591008599847555\n",
      "0.025607537012547255\n",
      "0.025457092095166445\n",
      "0.025528861908242106\n",
      "0.02550719492137432\n",
      "0.02563505293801427\n",
      "0.025488192914053798\n",
      "0.025483758887276053\n",
      "0.025740141980350018\n",
      "0.02560295886360109\n",
      "0.02549529611133039\n",
      "0.025596466846764088\n",
      "0.025489964056760073\n",
      "0.025511502055451274\n",
      "0.025509921135380864\n",
      "0.02548606786876917\n",
      "0.025783491786569357\n",
      "0.025524178985506296\n",
      "0.02555558900348842\n",
      "0.025562455877661705\n",
      "0.025461555924266577\n",
      "0.0254846119787544\n",
      "0.02546462882310152\n",
      "0.025538089917972684\n",
      "0.02586074103601277\n",
      "0.025495159905403852\n",
      "0.025557614862918854\n",
      "0.02551602595485747\n",
      "0.02560573210939765\n",
      "0.025502836098894477\n",
      "0.025434138951823115\n",
      "0.025547697907313704\n",
      "0.025931899901479483\n",
      "0.02573638199828565\n",
      "0.025544492062181234\n",
      "0.025529808131977916\n",
      "0.02557327994145453\n",
      "0.02548974403180182\n",
      "0.025622163899242878\n",
      "0.02546455105766654\n",
      "0.025832094019278884\n",
      "0.02548738196492195\n",
      "0.02558027091436088\n",
      "0.025561504997313023\n",
      "0.02558532706461847\n",
      "0.02551315911114216\n",
      "0.025469596032053232\n",
      "0.025607137940824032\n",
      "0.025870261015370488\n",
      "0.025505747878924012\n",
      "0.02548936801031232\n",
      "0.025486516067758203\n",
      "0.02553265285678208\n",
      "0.02554264711216092\n",
      "0.025487750070169568\n",
      "0.02549371891655028\n",
      "0.025921614142134786\n",
      "0.025553255109116435\n",
      "0.02558028604835272\n",
      "0.0254808000754565\n",
      "0.027391661889851093\n",
      "0.02722457400523126\n",
      "0.025486649945378304\n",
      "0.025518055073916912\n",
      "0.025793773820623755\n",
      "0.025611140066757798\n",
      "0.025688369991257787\n",
      "0.025544464122503996\n",
      "0.025581418070942163\n",
      "0.02565073687583208\n",
      "0.026166684925556183\n",
      "0.02615224104374647\n",
      "0.02652975800447166\n",
      "0.02622937085106969\n",
      "0.026204796973615885\n",
      "0.02622869610786438\n",
      "0.026229880983009934\n",
      "0.026347829960286617\n",
      "0.02616524090990424\n",
      "0.026183123933151364\n",
      "0.02644359297119081\n",
      "0.026196437887847424\n",
      "0.02624694094993174\n",
      "0.02618033206090331\n",
      "0.02619037590920925\n",
      "0.026172026991844177\n",
      "0.02622585603967309\n",
      "0.026203589979559183\n",
      "0.026439165929332376\n",
      "0.02621258795261383\n",
      "0.02620037505403161\n",
      "0.026194724952802062\n",
      "0.026251343078911304\n",
      "0.02619236591272056\n",
      "0.02616798784583807\n",
      "0.026237904094159603\n",
      "0.02642774092964828\n",
      "0.026218769140541553\n",
      "0.026212694123387337\n",
      "0.026214053155854344\n",
      "0.026170399971306324\n",
      "0.026154377963393927\n",
      "0.026199536863714457\n",
      "0.026206800946965814\n",
      "0.026413880055770278\n",
      "0.026262008119374514\n",
      "0.026222517946735024\n",
      "0.02618380612693727\n",
      "0.026211123913526535\n",
      "0.026239298982545733\n",
      "0.026219130028039217\n",
      "0.026185621973127127\n",
      "0.026432285085320473\n",
      "0.026174080790951848\n",
      "0.02613425417803228\n",
      "0.026180842891335487\n",
      "0.026178821921348572\n",
      "0.026210287120193243\n",
      "0.026158398017287254\n",
      "0.02613959601148963\n",
      "0.026478769024834037\n",
      "0.02600363502278924\n",
      "0.02560166409239173\n",
      "0.02555973012931645\n",
      "0.025619541062042117\n",
      "0.025501648895442486\n",
      "0.02556383004412055\n",
      "0.025489296996966004\n",
      "0.025794593850150704\n",
      "0.025479353964328766\n",
      "0.025557214859873056\n",
      "0.025620711036026478\n",
      "0.025453316047787666\n",
      "0.02556029916740954\n",
      "0.0254786207806319\n",
      "0.025543306954205036\n",
      "0.025798637187108397\n",
      "0.0254682470113039\n",
      "0.02547826897352934\n",
      "0.02553050289861858\n",
      "0.0256507929880172\n",
      "0.025504309916868806\n",
      "0.02551508485339582\n",
      "0.025606172159314156\n",
      "0.025770396925508976\n",
      "0.025828800862655044\n",
      "0.025653230026364326\n",
      "0.025514377979561687\n",
      "0.025568912969902158\n",
      "0.02550698583945632\n",
      "0.02563146105967462\n",
      "0.025433850940316916\n",
      "0.02586581208743155\n",
      "0.025490497006103396\n",
      "0.025658388854935765\n",
      "0.025533505016937852\n",
      "0.02557924692519009\n",
      "0.025421037105843425\n",
      "0.025615420890972018\n",
      "0.02564833709038794\n",
      "0.025794427143409848\n",
      "0.025560242123901844\n",
      "0.02551410300657153\n",
      "0.02553524705581367\n",
      "2.685906467959285\n"
     ]
    }
   ],
   "source": [
    "# Shell 2D\n",
    "num_samples = [100] #[100, 200, 500, 1000, 2000]\n",
    "twice_dim = 4\n",
    "init = np.random.randn(twice_dim//2)*2\n",
    "traj_length = 10\n",
    "traj_step_size = 0.01\n",
    "num_of_runs = 20\n",
    "potential = '2d_shell'\n",
    "potential_function = Shell2D\n",
    "func = torch.load(f'{potential}/model')\n",
    "func_mod = ODEFunc_exact_den(func)\n",
    "\n",
    "start = time.perf_counter()        \n",
    "for q in num_samples:\n",
    "    for i in range(1, num_of_runs+1):\n",
    "        samps,trajs,energies,acceptance = Neural_HMC(potential_function, init, q, traj_step_size, traj_length, store=True)\n",
    "\n",
    "#         if not os.path.exists(f'{potential}/neural_{q}'):\n",
    "#             os.makedirs(f'{potential}/neural_{q}')\n",
    "\n",
    "#         with open(f'{potential}/neural_{q}/{i}_hmc_samps.npy', 'wb') as f:\n",
    "#             np.save(f, samps)\n",
    "\n",
    "#         with open(f'{potential}/neural_{q}/{i}_info.npy', 'wb') as f:\n",
    "#             np.save(f, np.array([time.perf_counter() - start, acceptance]))\n",
    "#         print(q, i)\n",
    "print(time.perf_counter() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian 10D\n",
    "num_samples = [10000]#[100, 200, 500, 1000, 2000]\n",
    "twice_dim = 20\n",
    "init = np.random.randn(twice_dim//2)*2\n",
    "traj_length = 10\n",
    "traj_step_size = 0.1\n",
    "num_of_runs = 5\n",
    "potential = '10d_gaussian'\n",
    "potential_function = GaussianXD\n",
    "func = torch.load(f'{potential}/model')\n",
    "func_mod = ODEFunc_exact_den(func)\n",
    "\n",
    "for q in num_samples:\n",
    "    for i in range(1, num_of_runs+1):\n",
    "        start = time.perf_counter()\n",
    "        samps,trajs,energies,acceptance = Neural_HMC(potential_function, init, q, traj_step_size, traj_length, store=True)\n",
    "\n",
    "        if not os.path.exists(f'{potential}/neural_{q}'):\n",
    "            os.makedirs(f'{potential}/neural_{q}')\n",
    "\n",
    "        with open(f'{potential}/neural_{q}/{i}_hmc_samps.npy', 'wb') as f:\n",
    "            np.save(f, samps)\n",
    "\n",
    "        with open(f'{potential}/neural_{q}/{i}_info.npy', 'wb') as f:\n",
    "            np.save(f, np.array([time.perf_counter() - start, acceptance]))\n",
    "        print(q, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d185b1a50fdb88fec028d16f50bb5bdddee526b509af9d3742231a76a4662799"
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-3320]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
